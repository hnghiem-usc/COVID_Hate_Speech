{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBerta_LSTM_Vigen_target_transfer","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pp3_2ZLq6NS","executionInfo":{"status":"ok","timestamp":1631166330123,"user_tz":420,"elapsed":7006,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"c66a7f6e-86fa-4dff-84dc-ea7a26257caa"},"source":["from datetime import datetime\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import re, string\n","import time\n","import random\n","import gc\n","import os, sys\n","import json\n","from copy import deepcopy\n","\n","#####\n","# !pip install gdown\n","!pip install transformers\n","pd.options.display.max_colwidth = 500\n","#####\n","\n","import torch\n","\n","from transformers import BertForSequenceClassification, AdamW , BertConfig, BertModel\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import LineByLineTextDataset\n","\n","from torch.nn import LSTM, Dropout, Linear, BCELoss, CrossEntropyLoss, Softmax, ReLU, LeakyReLU, Tanh\n","from torch.nn.functional import softmax, relu\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.utils.data.dataset import Subset\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n","\n","%load_ext autoreload \n","%autoreload 2"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzEvkGACWMqX","executionInfo":{"status":"ok","timestamp":1631166330127,"user_tz":420,"elapsed":22,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"9a4ab6ac-063a-4a14-fa53-2e0f0204f0ed"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUYifLXWu1D0","executionInfo":{"status":"ok","timestamp":1631166330128,"user_tz":420,"elapsed":19,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"0faa17ed-a943-48a3-87e5-3cb6cadf577d"},"source":["!ls \"/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model\""],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["'Bert Models'   models.py   __pycache__   RoBerta  'Roberta Final'\n"]}]},{"cell_type":"code","metadata":{"id":"f6srHxsX4FhM","executionInfo":{"status":"ok","timestamp":1631166330128,"user_tz":420,"elapsed":16,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["PATH_PREFIX = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8keHP8s6Fj4","executionInfo":{"status":"ok","timestamp":1631166330647,"user_tz":420,"elapsed":534,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils')\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model')\n","\n","from models import *\n","from text_processing import *"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFayi-JvrDHx","executionInfo":{"status":"ok","timestamp":1631166331277,"user_tz":420,"elapsed":633,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"c856d94e-0d50-403c-b0bf-d5000723690b"},"source":["# Enable GPU before proceeding\n","device_name = tf.test.gpu_device_name()\n","print(\"Device found:\", device_name)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Device found: /device:GPU:0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLkb-vZhryC-","executionInfo":{"status":"ok","timestamp":1631166331278,"user_tz":420,"elapsed":8,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"381f3486-b988-4572-dba3-9763560c5c36"},"source":["# Test if CUDA is available, use it\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","print(\"Using CUDA Device:\", torch.cuda.get_device_name())"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA Device: Tesla K80\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jgmig-gyT5_U"},"source":["# DOWNLOAD AND PREPROCESS DATA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIGMfizwRlrM","executionInfo":{"status":"ok","timestamp":1631166331449,"user_tz":420,"elapsed":176,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"9c4b6487-eb2f-45bb-8c39-9ad89566bdc7"},"source":["train_dt, test_dt = load_data(PATH_PREFIX + '/Data/HER_final_data.csv', \n","                   dtype_dict={'tweetid':str, 'label':int, 'target':int, 'aggr':int})"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["label\n","0    1894\n","1     304\n","2     202\n","Name: label, dtype: int64\n","target\n","0    2003\n","1     367\n","2      25\n","3       5\n","Name: target, dtype: int64\n","aggr\n","0    1799\n","1     533\n","2      68\n","Name: aggr, dtype: int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"42nIFpyRNTJw"},"source":["## Tokenize state"]},{"cell_type":"code","metadata":{"id":"GPIccBVB_yr1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631166331450,"user_tz":420,"elapsed":7,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"4e72e443-5bf3-42cf-fd5f-66295f41ebcd"},"source":["# import hashtag_dict \n","HASHTAG_DIR = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils/' \n","with open(HASHTAG_DIR + 'hashtag_dict.json', 'r') as f:\n","    hashtag_dict = json.load(f)\n","\n","# CUDA is memory hungry. Needs to reduce the max_len\n","MAX_LEN = 225\n","print(\"Maximum length of cleaned up dataset is:\", MAX_LEN)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum length of cleaned up dataset is: 225\n"]}]},{"cell_type":"code","metadata":{"id":"CIPViSOuXhTA","executionInfo":{"status":"ok","timestamp":1631166332677,"user_tz":420,"elapsed":1230,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HP_qSMkuohrO","executionInfo":{"status":"ok","timestamp":1631166334880,"user_tz":420,"elapsed":2206,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"2f6a75ac-7ab7-4358-81eb-557a3447df09"},"source":["train_input_ids, train_attn_masks  = tokenizing(train_dt.text.values, tokenizer, MAX_LEN)\n","test_input_ids, test_attn_masks  = tokenizing(test_dt.text.values, tokenizer, MAX_LEN)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Total time taken:  1.726388692855835\n","Total time taken:  0.4038052558898926\n"]}]},{"cell_type":"code","metadata":{"id":"5UaN8FNEUbn9","executionInfo":{"status":"ok","timestamp":1631166334881,"user_tz":420,"elapsed":6,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["varlist = ['label', 'target', 'aggr']\n","_, train_tensor = arrange_tensor(train_dt, train_input_ids, train_attn_masks ,varlist=varlist)\n","_, test_tensor  = arrange_tensor(test_dt, test_input_ids, test_attn_masks ,varlist=varlist)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RNJyuEsVfCB","executionInfo":{"status":"ok","timestamp":1631166335064,"user_tz":420,"elapsed":188,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"9baeadfa-9638-449c-be04-6b4f454da115"},"source":["test_tensor[:][-1].shape"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([600])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"jKEuQnyKzKaP"},"source":["# BUILD MODEL"]},{"cell_type":"markdown","metadata":{"id":"ptpLoOHEKWBx"},"source":["In this model, we will build an LSTM model leveraging BERT base hidden data on top. The key difference compared to simpleBert is that we need to save the parameters."]},{"cell_type":"code","metadata":{"id":"t0NtCmP8NM8B","executionInfo":{"status":"ok","timestamp":1631166335064,"user_tz":420,"elapsed":2,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Saved Parameters/\""],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"2K9sRD_qgCEY","executionInfo":{"status":"ok","timestamp":1631166335202,"user_tz":420,"elapsed":140,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["def run_models(save_path:str, model_class, num_labels, num_tasks, max_len,\n","        num_epochs, loss_funcs:list, device,\n","        train_tensor, test_tensor, val_tensor=None, batch_size=20, label_ls = [[0,1,2]],\n","        seed_list = [101,211,307,401,503], save_seed=0,\n","        transfer_weights:str=None):\n","\n","    final_metrics = dict()\n","    # Iterate through seeds\n","    for idx, s in enumerate(seed_list):\n","        print('========== Trying seed', s, '==========')\n","        # Reset the seed\n","        seed = s\n","        random.seed(seed)\n","        os.environ['PYTHONHASHSEED'] = str(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = True\n","    \n","        # print(\"Random num: \", random.randint(1,1000))\n","        \n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        # Initialize model\n","        model = model_class('roberta-base', num_labels=num_labels, max_len=max_len)\n","        # load transfer weights:\n","        if transfer_weights is not None:\n","            base_model = model_class.load(transfer_weights)\n","            model.load_states(base_model.device_objects)\n","\n","        optimizer = AdamW(model.parameters(),lr=2e-5, eps=1e-6, weight_decay=1e-3)\n","        build_params = {'device':device, 'optimizer': optimizer, 'loss_functions': loss_funcs}\n","        # build \n","        model.build(**build_params)\n","\n","        # train\n","        if val_tensor is None:\n","            model.train(train_tensor, batch_size = batch_size,\n","                        num_epochs=num_epochs, verbose=True)\n","        else: \n","            model.train(train_tensor, val_tensor=val_tensor, batch_size=batch_size,\n","                        num_epochs=num_epochs, verbose=True)\n","\n","        # evaluate\n","        _, logits, labels = model.evaluate(test_tensor)\n","        logits, labels = format_outputs(logits, labels, num_tasks)\n","        preds = get_labels_from_multi_logits(logits)\n","        res = print_report_multitask(labels, preds, label_ls )\n","\n","        final_metrics[s] = res\n","\n","        if idx==save_seed:\n","            model.save(save_path)\n","        del model\n","\n","    return final_metrics "],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTFb-MmerbQ6"},"source":[""]},{"cell_type":"code","metadata":{"id":"15EW5Jz3HNU-","executionInfo":{"status":"ok","timestamp":1631166335203,"user_tz":420,"elapsed":4,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["##### load the correct weights########\n","TRANSFER_DIR = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Saved Parameters/Vidgen/Target/\""],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_iTm811coQuf","executionInfo":{"status":"ok","timestamp":1631166335333,"user_tz":420,"elapsed":133,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"7c9582cd-a881-4a82-c659-84e6b5928dba"},"source":["###### !!CHECK THIS!! ######\n","model_class = RobertaLSTM\n","model_str = 'Roberta_LSTM'\n","###########################\n","for filename in os.listdir(TRANSFER_DIR):\n","    if model_str  in filename:\n","        # pick the first file\n","        transfer_weights = TRANSFER_DIR + filename\n","        print(\"Transferring weights from :\", transfer_weights)\n","        break"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Transferring weights from : /content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Saved Parameters/Vidgen/Target/Roberta_LSTM_09_09_21_0538.pth\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W10NDfiEIa6S","executionInfo":{"status":"ok","timestamp":1631169204302,"user_tz":420,"elapsed":2868971,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"b6a66adc-416f-4b4a-d61e-3320b5b8a8f9"},"source":["target_ls = ['label','target','aggr']\n","num_labels =  [3,4,3]\n","num_tasks = len(num_labels)\n","label_ls = [[0,1,2],[0,1,2,3],[0,1,2]]\n","num_epochs = 3\n","# change correct weights\n","weight_dict = calculate_class_weights(train_dt, target_ls)\n","weight_ls  = [list(weight_dict[k].values()) for k in target_ls]\n","loss_funcs = [CrossEntropyLoss(weight=torch.tensor(w).to(device)) for w in weight_ls]\n","\n","model_params = {'num_labels':num_labels, 'num_tasks':num_tasks, \n","                'max_len':MAX_LEN,'label_ls':label_ls,\n","                'num_epochs':num_epochs, 'loss_funcs':loss_funcs,\n","                'transfer_weights': transfer_weights\n","                }\n","\n","results = run_models(save_path = SAVE_PATH, model_class=model_class, device=device, \n","     train_tensor=train_tensor, test_tensor=test_tensor, \n","    #  seed_list=[101,211],\n","      **model_params )"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["========== Trying seed 101 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 2 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 2 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Loading state_dict for self.bert\n","Loading state_dict for self.dropout\n","Loading state_dict for self.lstm\n","Loading state_dict for self.fc0\n","Loading state_dict for self.fc1\n","Running Epoch  0\n","Training loss : [115.33969154953957, 111.81412996351719, 101.52368620038033]. Validation loss: []. Duration: 181.93715834617615\n","Running Epoch  1\n","Training loss : [81.445781737566, 70.37888374179602, 72.96742022037506]. Validation loss: []. Duration: 181.7611210346222\n","Running Epoch  2\n","Training loss : [70.1278098076582, 51.2244425304234, 57.011986032128334]. Validation loss: []. Duration: 182.39418768882751\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.92688172 0.39583333 0.36781609]\n","recall   : [0.88865979 0.24358974 0.86486486]\n","fscore   : [0.90736842 0.3015873  0.51612903]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.96696035 0.58088235 0.2        0.        ]\n","recall   : [0.87450199 0.90804598 0.2        0.        ]\n","fscore   : [0.91841004 0.70852018 0.2        0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.8836689  0.58865248 0.33333333]\n","recall   : [0.89366516 0.60144928 0.2       ]\n","fscore   : [0.88863892 0.59498208 0.25      ]\n","support  : [442 138  20]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["========== Trying seed 211 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 2 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 2 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Loading state_dict for self.bert\n","Loading state_dict for self.dropout\n","Loading state_dict for self.lstm\n","Loading state_dict for self.fc0\n","Loading state_dict for self.fc1\n","Running Epoch  0\n","Training loss : [113.78715500235558, 110.23716909438372, 101.17513817548752]. Validation loss: []. Duration: 182.31930232048035\n","Running Epoch  1\n","Training loss : [82.58388315141201, 70.49364296719432, 74.53562885522842]. Validation loss: []. Duration: 182.15012907981873\n","Running Epoch  2\n","Training loss : [74.33534881472588, 44.342558689415455, 56.96532618999481]. Validation loss: []. Duration: 181.69722890853882\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.95205479 0.4137931  0.28846154]\n","recall   : [0.85979381 0.30769231 0.81081081]\n","fscore   : [0.9035753  0.35294118 0.42553191]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.97052154 0.57553957 0.15       0.        ]\n","recall   : [0.85258964 0.91954023 0.3        0.        ]\n","fscore   : [0.90774125 0.7079646  0.2        0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.90300231 0.61486486 0.36842105]\n","recall   : [0.88461538 0.65942029 0.35      ]\n","fscore   : [0.89371429 0.63636364 0.35897436]\n","support  : [442 138  20]\n","========== Trying seed 307 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 2 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 2 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Loading state_dict for self.bert\n","Loading state_dict for self.dropout\n","Loading state_dict for self.lstm\n","Loading state_dict for self.fc0\n","Loading state_dict for self.fc1\n","Running Epoch  0\n","Training loss : [115.32690474390984, 109.5954211987555, 103.38130986690521]. Validation loss: []. Duration: 182.48148608207703\n","Running Epoch  1\n","Training loss : [83.72731620073318, 71.20186262577772, 76.31431975960732]. Validation loss: []. Duration: 181.68836760520935\n","Running Epoch  2\n","Training loss : [76.17680682241917, 49.751807348802686, 54.804770573973656]. Validation loss: []. Duration: 182.757967710495\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.94183445 0.34615385 0.31683168]\n","recall   : [0.86804124 0.23076923 0.86486486]\n","fscore   : [0.90343348 0.27692308 0.46376812]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.96086957 0.59090909 0.125      0.        ]\n","recall   : [0.88047809 0.89655172 0.1        0.        ]\n","fscore   : [0.91891892 0.71232877 0.11111111 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.90163934 0.57763975 0.41666667]\n","recall   : [0.87104072 0.67391304 0.25      ]\n","fscore   : [0.88607595 0.62207358 0.3125    ]\n","support  : [442 138  20]\n","========== Trying seed 401 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 2 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 2 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Loading state_dict for self.bert\n","Loading state_dict for self.dropout\n","Loading state_dict for self.lstm\n","Loading state_dict for self.fc0\n","Loading state_dict for self.fc1\n","Running Epoch  0\n","Training loss : [112.9478343129158, 105.89070641621947, 101.81613665819168]. Validation loss: []. Duration: 182.15459609031677\n","Running Epoch  1\n","Training loss : [83.41961067914963, 69.30554784089327, 74.88250494003296]. Validation loss: []. Duration: 182.22853112220764\n","Running Epoch  2\n","Training loss : [73.0271305590868, 53.01106731966138, 62.11612020432949]. Validation loss: []. Duration: 182.06878209114075\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.9478458  0.35294118 0.36486486]\n","recall   : [0.86185567 0.38461538 0.72972973]\n","fscore   : [0.90280778 0.36809816 0.48648649]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.95555556 0.61864407 0.12903226 0.        ]\n","recall   : [0.85657371 0.83908046 0.4        0.        ]\n","fscore   : [0.90336134 0.71219512 0.19512195 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.91079812 0.58940397 0.26086957]\n","recall   : [0.87782805 0.64492754 0.3       ]\n","fscore   : [0.89400922 0.61591696 0.27906977]\n","support  : [442 138  20]\n","========== Trying seed 503 ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 2 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 2 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Loading state_dict for self.bert\n","Loading state_dict for self.dropout\n","Loading state_dict for self.lstm\n","Loading state_dict for self.fc0\n","Loading state_dict for self.fc1\n","Running Epoch  0\n","Training loss : [114.11950632929802, 108.544452752918, 100.8031949698925]. Validation loss: []. Duration: 182.07388043403625\n","Running Epoch  1\n","Training loss : [83.05455918610096, 74.01197358965874, 75.25594517588615]. Validation loss: []. Duration: 181.8640217781067\n","Running Epoch  2\n","Training loss : [72.3975070565939, 50.45725153386593, 55.82819390296936]. Validation loss: []. Duration: 181.97975492477417\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.94606742 0.53846154 0.29310345]\n","recall   : [0.86804124 0.26923077 0.91891892]\n","fscore   : [0.90537634 0.35897436 0.44444444]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.97963801 0.55555556 0.4        0.        ]\n","recall   : [0.8625498  0.97701149 0.2        0.        ]\n","fscore   : [0.91737288 0.70833333 0.26666667 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.90137615 0.60927152 0.53846154]\n","recall   : [0.88914027 0.66666667 0.35      ]\n","fscore   : [0.8952164  0.6366782  0.42424242]\n","support  : [442 138  20]\n"]}]},{"cell_type":"code","metadata":{"id":"HPpGBDuFRMcl","executionInfo":{"status":"ok","timestamp":1631169204593,"user_tz":420,"elapsed":310,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["RESULT_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Results/\"\n","\n","specific_res = get_specific_results(results)\n","specific_res.to_pickle(RESULT_PATH + model_str + '_targetvidgen_specific.pkl')\n","\n","macro_res    = get_macro_results(results)\n","macro_res.to_pickle(RESULT_PATH + model_str + '_targetvidgen_macro.pkl')"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3V4slhXyCB_","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1631169204594,"user_tz":420,"elapsed":11,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"ecd787e7-3580-4df8-cc38-0bf1ba99f77d"},"source":["specific_res"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Normal</th>\n","      <th>Abusive</th>\n","      <th>Hate</th>\n","      <th>Neither/NA</th>\n","      <th>Anti-Asian</th>\n","      <th>Anti-Black</th>\n","      <th>Both</th>\n","      <th>No.Agg.</th>\n","      <th>Some.Agg.</th>\n","      <th>Very.Agg.</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.94 +- 0.01</td>\n","      <td>0.41 +- 0.07</td>\n","      <td>0.33 +- 0.03</td>\n","      <td>0.97 +- 0.01</td>\n","      <td>0.58 +- 0.02</td>\n","      <td>0.2 +- 0.1</td>\n","      <td>--</td>\n","      <td>0.9 +- 0.01</td>\n","      <td>0.6 +- 0.01</td>\n","      <td>0.38 +- 0.09</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.87 +- 0.01</td>\n","      <td>0.29 +- 0.06</td>\n","      <td>0.84 +- 0.06</td>\n","      <td>0.87 +- 0.01</td>\n","      <td>0.91 +- 0.04</td>\n","      <td>0.24 +- 0.1</td>\n","      <td>--</td>\n","      <td>0.88 +- 0.01</td>\n","      <td>0.65 +- 0.03</td>\n","      <td>0.29 +- 0.06</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.9 +- 0.0</td>\n","      <td>0.33 +- 0.04</td>\n","      <td>0.47 +- 0.03</td>\n","      <td>0.91 +- 0.01</td>\n","      <td>0.71 +- 0.0</td>\n","      <td>0.19 +- 0.05</td>\n","      <td>--</td>\n","      <td>0.89 +- 0.0</td>\n","      <td>0.62 +- 0.02</td>\n","      <td>0.32 +- 0.06</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Normal       Abusive  ...     Some.Agg.     Very.Agg.\n","0  0.94 +- 0.01  0.41 +- 0.07  ...   0.6 +- 0.01  0.38 +- 0.09\n","1  0.87 +- 0.01  0.29 +- 0.06  ...  0.65 +- 0.03  0.29 +- 0.06\n","2    0.9 +- 0.0  0.33 +- 0.04  ...  0.62 +- 0.02  0.32 +- 0.06\n","\n","[3 rows x 10 columns]"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"id":"0Mc699tAMUCM","executionInfo":{"status":"ok","timestamp":1631169204792,"user_tz":420,"elapsed":205,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"2b6a5a67-9949-4129-9e2e-3a4de98691d3"},"source":["macro_res"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.56 +- 0.02</td>\n","      <td>0.66 +- 0.01</td>\n","      <td>0.61 +- 0.02</td>\n","      <td>0.78 +- 0.01</td>\n","      <td>0.89 +- 0.02</td>\n","      <td>0.83 +- 0.01</td>\n","      <td>0.63 +- 0.03</td>\n","      <td>0.61 +- 0.03</td>\n","      <td>0.62 +- 0.03</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              0             1  ...             7             8\n","0  0.56 +- 0.02  0.66 +- 0.01  ...  0.61 +- 0.03  0.62 +- 0.03\n","\n","[1 rows x 9 columns]"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"8H3mmM5zMY5v","executionInfo":{"status":"ok","timestamp":1631169204793,"user_tz":420,"elapsed":10,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":[""],"execution_count":21,"outputs":[]}]}