{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBerta_LSTM_Founta_analysis_equal","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xcEsVXtti4Xb","executionInfo":{"status":"ok","timestamp":1631394698641,"user_tz":420,"elapsed":374,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["## This module performs cross-training analyses on Founta and ours by selecting\n","# an equal number of observations \n","# !!! MAKE SURE TO HAVE TESLA K80\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pp3_2ZLq6NS","executionInfo":{"status":"ok","timestamp":1631394707448,"user_tz":420,"elapsed":8367,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"2986474c-55b5-4b72-a3be-7ed5b69cfe62"},"source":["from datetime import datetime\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import re, string\n","import time\n","import random\n","import gc\n","import os, sys\n","import json\n","from copy import deepcopy\n","\n","#####\n","# !pip install gdown\n","!pip install transformers\n","pd.options.display.max_colwidth = 500\n","#####\n","\n","import torch\n","\n","from transformers import BertForSequenceClassification, AdamW , BertConfig, BertModel\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import LineByLineTextDataset\n","\n","from torch.nn import LSTM, Dropout, Linear, BCELoss, CrossEntropyLoss, Softmax, ReLU, LeakyReLU, Tanh\n","from torch.nn.functional import softmax, relu\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.utils.data.dataset import Subset\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n","\n","seed = 123\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = True\n","\n","%load_ext autoreload \n","%autoreload 2"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzEvkGACWMqX","executionInfo":{"status":"ok","timestamp":1631394707449,"user_tz":420,"elapsed":16,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"ee559cf1-adc7-42d2-a781-d69133e4fb4e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUYifLXWu1D0","executionInfo":{"status":"ok","timestamp":1631394707452,"user_tz":420,"elapsed":14,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"67e51e26-3087-4f1e-b15b-46a0696efbb7"},"source":["!ls \"/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model\""],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["'Bert Models'   models.py   __pycache__   RoBerta  'Roberta Final'\n"]}]},{"cell_type":"code","metadata":{"id":"f6srHxsX4FhM","executionInfo":{"status":"ok","timestamp":1631394707453,"user_tz":420,"elapsed":10,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["PATH_PREFIX = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8keHP8s6Fj4","executionInfo":{"status":"ok","timestamp":1631394708248,"user_tz":420,"elapsed":805,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils')\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model')\n","\n","from models import *\n","from text_processing import *"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFayi-JvrDHx","executionInfo":{"status":"ok","timestamp":1631394708946,"user_tz":420,"elapsed":700,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"69752985-57bf-40af-8c25-1fecf12ccf93"},"source":["# Enable GPU before proceeding\n","device_name = tf.test.gpu_device_name()\n","print(\"Device found:\", device_name)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Device found: /device:GPU:0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLkb-vZhryC-","executionInfo":{"status":"ok","timestamp":1631394708946,"user_tz":420,"elapsed":17,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"ff7fd8f8-b961-4ca0-a60c-cf4ed09c566d"},"source":["# Test if CUDA is available, use it\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","print(\"Using CUDA Device:\", torch.cuda.get_device_name())"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA Device: Tesla K80\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jgmig-gyT5_U"},"source":["# DOWNLOAD AND PREPROCESS DATA"]},{"cell_type":"code","metadata":{"id":"CUuv0a5Dn4OQ","executionInfo":{"status":"ok","timestamp":1631394708947,"user_tz":420,"elapsed":14,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["n_sample = 200"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RXsD6lkfpkZt","executionInfo":{"status":"ok","timestamp":1631394708948,"user_tz":420,"elapsed":14,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"29fe800c-0f59-4917-875a-5af6893101b6"},"source":["her = pd.read_csv(PATH_PREFIX + '/Data/HER_final_data.csv', \n","                   dtype={'tweetid':str, 'label':int, 'target':int, 'aggr':int})\n","\n","her = her.groupby('label').apply(lambda x: x.sample(n_sample) )\n","her.reset_index(drop=True, inplace=True)\n","her = her.sample(frac=1, random_state=seed)\n","\n","Counter(her.label)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({0: 200, 1: 200, 2: 200})"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIGMfizwRlrM","executionInfo":{"status":"ok","timestamp":1631394708949,"user_tz":420,"elapsed":11,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"fcab9d53-7c3c-4e2e-a30c-895d6c0ec767"},"source":["ft = pd.read_csv(PATH_PREFIX + '/Data/founta_final_data.csv', \n","                   dtype={'tweetid':str, 'label':int, 'target':int, 'aggr':int})\n","ft['id'] = ft.index\n","ft = ft.groupby('label').apply(lambda x: x.sample(n_sample) )\n","ft.reset_index(drop=True, inplace=True)\n","ft = ft.sample(frac=1, random_state=seed)\n","\n","Counter(ft.label)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Counter({0: 200, 1: 200, 2: 200})"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"fj1AgMJUn99j","executionInfo":{"status":"ok","timestamp":1631394708950,"user_tz":420,"elapsed":9,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["## USE FOUNTA MODEL TO TEST ON OURS "],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42nIFpyRNTJw"},"source":["## Tokenize state"]},{"cell_type":"code","metadata":{"id":"GPIccBVB_yr1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631394709284,"user_tz":420,"elapsed":342,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"6af17ae5-ce80-43be-bece-8fc0283788d7"},"source":["# import hashtag_dict \n","HASHTAG_DIR = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils/' \n","with open(HASHTAG_DIR + 'hashtag_dict.json', 'r') as f:\n","    hashtag_dict = json.load(f)\n","\n","# CUDA is memory hungry. Needs to reduce the max_len\n","MAX_LEN = 225\n","print(\"Maximum length of cleaned up dataset is:\", MAX_LEN)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum length of cleaned up dataset is: 225\n"]}]},{"cell_type":"code","metadata":{"id":"CIPViSOuXhTA","executionInfo":{"status":"ok","timestamp":1631394715515,"user_tz":420,"elapsed":6234,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HP_qSMkuohrO","executionInfo":{"status":"ok","timestamp":1631394716364,"user_tz":420,"elapsed":862,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"73941a4f-b799-487c-cc78-b4037df78f53"},"source":["varlist = ['label']\n","test_input_ids, test_attn_masks  = tokenizing(her.text.values, tokenizer, MAX_LEN)\n","_, test_tensor  = arrange_tensor(her, test_input_ids, test_attn_masks ,varlist=varlist)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Total time taken:  0.5104415416717529\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RNJyuEsVfCB","executionInfo":{"status":"ok","timestamp":1631394716366,"user_tz":420,"elapsed":21,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"95183ab9-3b26-4e68-ad45-bfafbfbe63ad"},"source":["test_tensor[:][-1].shape"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([600])"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"jKEuQnyKzKaP"},"source":["## BUILD MODEL"]},{"cell_type":"markdown","metadata":{"id":"ptpLoOHEKWBx"},"source":["In this model, we will build an LSTM model leveraging BERT base hidden data on top. The key difference compared to simpleBert is that we need to save the parameters."]},{"cell_type":"code","metadata":{"id":"t0NtCmP8NM8B","executionInfo":{"status":"ok","timestamp":1631394716368,"user_tz":420,"elapsed":18,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Saved Parameters/\""],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTFb-MmerbQ6"},"source":[""]},{"cell_type":"code","metadata":{"id":"15EW5Jz3HNU-","executionInfo":{"status":"ok","timestamp":1631394716369,"user_tz":420,"elapsed":18,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["##### load the correct weights########\n","TRANSFER_DIR = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Saved Parameters/Founta/\""],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_iTm811coQuf","executionInfo":{"status":"ok","timestamp":1631394716370,"user_tz":420,"elapsed":17,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"55b896a0-dfb9-42d5-da2c-55303b0aba00"},"source":["###### !!CHECK THIS!! ######\n","model_class = RobertaLSTM\n","model_str = 'Roberta_LSTM'\n","###########################\n","for filename in os.listdir(TRANSFER_DIR):\n","    if model_str  in filename:\n","        # pick the first file\n","        transfer_weights = TRANSFER_DIR + filename\n","        print(\"Transferring weights from :\", transfer_weights)\n","        break"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Transferring weights from : /content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Saved Parameters/Founta/Roberta_LSTM_09_07_21_2203.pth\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W10NDfiEIa6S","executionInfo":{"status":"ok","timestamp":1631394737077,"user_tz":420,"elapsed":20719,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"1f0f4fda-93ee-441e-f87b-410f0bbcfc00"},"source":["target_ls = ['label']\n","num_labels =  [3]\n","num_tasks = len(num_labels)\n","label_ls = [[0,1,2]]\n","num_epochs = 3\n","max_len = 225\n","\n","# change correct weights\n","loss_funcs = [CrossEntropyLoss()] * num_tasks\n","\n","model = model_class('roberta-base', num_labels=num_labels, max_len=max_len)\n","# load transfer weights:\n","if transfer_weights is not None:\n","    base_model = model_class.load(transfer_weights)\n","    model.load_states(base_model.device_objects)\n","\n","optimizer = AdamW(model.parameters(),lr=2e-5, eps=1e-6, weight_decay=1e-3)\n","build_params = {'device':device, 'optimizer': optimizer, 'loss_functions': loss_funcs}\n","# build \n","model.build(**build_params)"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Sigle-task training\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Single-task training\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Sigle-task training\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Single-task training\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Loading state_dict for self.bert\n","Loading state_dict for self.dropout\n","Loading state_dict for self.lstm\n","Loading state_dict for self.fc0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9JOXCzdToCuB","executionInfo":{"status":"ok","timestamp":1631394755079,"user_tz":420,"elapsed":18016,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"cf0540bd-fe9f-416b-bb1f-8273b021e8e7"},"source":[" # evaluate\n","_, logits, labels = model.evaluate(test_tensor)\n","logits, labels = format_outputs(logits, labels, num_tasks)\n","preds = get_labels_from_multi_logits(logits)\n","res = print_report_multitask(labels, preds, label_ls )\n","\n"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["num_tasks 1\n","\n","-----Report for task 0:-----\n","\n","precision: [0.45132743 0.61538462 0.32765957]\n","recall   : [0.765 0.08  0.385]\n","fscore   : [0.567718   0.14159292 0.35402299]\n","support  : [200 200 200]\n"]}]},{"cell_type":"code","metadata":{"id":"8H3mmM5zMY5v","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1631394755081,"user_tz":420,"elapsed":30,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"52848fb2-d35a-40d7-b9cc-36db4aa47c7c"},"source":["cm = pd.DataFrame(confusion_matrix(labels[0],  preds[0]), columns = ['Normal', 'Abusive', 'Hate'])\n","cm.index = ['Normal', 'Abusive', 'Hate']\n","cm"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Normal</th>\n","      <th>Abusive</th>\n","      <th>Hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Normal</th>\n","      <td>153</td>\n","      <td>2</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>Abusive</th>\n","      <td>71</td>\n","      <td>16</td>\n","      <td>113</td>\n","    </tr>\n","    <tr>\n","      <th>Hate</th>\n","      <td>115</td>\n","      <td>8</td>\n","      <td>77</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Normal  Abusive  Hate\n","Normal      153        2    45\n","Abusive      71       16   113\n","Hate        115        8    77"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lDav5b8WqMEW","executionInfo":{"status":"ok","timestamp":1631394755082,"user_tz":420,"elapsed":28,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"62a65375-f64a-4557-fc05-2a7a61ab9b1c"},"source":["# the array is of the right order\n","np.all(ft.label.values == np.array(labels[0]))"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"X-o7ZWBErRg6"},"source":["## 2. TRAINED ON OURS, CLASSIFY FOUNTA"]},{"cell_type":"markdown","metadata":{"id":"BH8dJnPLrosM"},"source":["## FOR LSTM MODEL"]},{"cell_type":"code","metadata":{"id":"Tml0CX1CxF3G","executionInfo":{"status":"ok","timestamp":1631394755685,"user_tz":420,"elapsed":627,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["del model, test_input_ids, test_attn_masks, test_tensor"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"QNDhl206rxoA","executionInfo":{"status":"ok","timestamp":1631394755687,"user_tz":420,"elapsed":10,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"02f53893-952d-4e7b-af94-518ac0481ff6"},"source":["LSTM_PATH = SAVE_PATH + '/Roberta/Roberta_LSTM_seed_401.pth'\n","LSTM_PATH"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Saved Parameters//Roberta/Roberta_LSTM_seed_401.pth'"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R1WEcnOfwmmr","executionInfo":{"status":"ok","timestamp":1631394756119,"user_tz":420,"elapsed":440,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"48871ca5-c04d-48ed-e130-b3925131dd03"},"source":["test_input_ids, test_attn_masks  = tokenizing(ft.text.values, tokenizer, MAX_LEN)\n","_, test_tensor  = arrange_tensor(ft, test_input_ids, test_attn_masks ,varlist=varlist)"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Total time taken:  0.3605804443359375\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CCQFXTkCu_MS","executionInfo":{"status":"ok","timestamp":1631394769927,"user_tz":420,"elapsed":13813,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"617edf82-bdca-449e-8832-2481c7dcefc4"},"source":["target_ls = ['label']\n","num_labels =  [3]\n","num_tasks = len(num_labels)\n","label_ls = [[0,1,2]]\n","num_epochs = 3\n","max_len = 225\n","\n","# change correct weights\n","loss_funcs = [CrossEntropyLoss()] * num_tasks\n","\n","model = model_class('roberta-base', num_labels=num_labels, max_len=max_len)\n","# load transfer weights:\n","if LSTM_PATH is not None:\n","    base_model = model_class.load(LSTM_PATH)\n","    model.load_states(base_model.device_objects)\n","\n","optimizer = AdamW(model.parameters(),lr=2e-5, eps=1e-6, weight_decay=1e-3)\n","build_params = {'device':device, 'optimizer': optimizer, 'loss_functions': loss_funcs}\n","# build \n","model.build(**build_params)"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Sigle-task training\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Single-task training\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Loading state_dict for self.bert\n","Loading state_dict for self.dropout\n","Loading state_dict for self.lstm\n","Loading state_dict for self.fc0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v24uHLjtx3NZ","executionInfo":{"status":"ok","timestamp":1631394787552,"user_tz":420,"elapsed":17640,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"8f73fdc9-1df1-4955-f0b4-fd52a52100ed"},"source":[" # evaluate\n","_, logits, labels = model.evaluate(test_tensor)\n","logits, labels = format_outputs(logits, labels, num_tasks)\n","preds = get_labels_from_multi_logits(logits)\n","res = print_report_multitask(labels, preds, label_ls )\n"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["num_tasks 1\n","\n","-----Report for task 0:-----\n","\n","precision: [0.48507463 0.51010101 0.        ]\n","recall   : [0.975 0.505 0.   ]\n","fscore   : [0.64784053 0.50753769 0.        ]\n","support  : [200 200 200]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"id":"1XwHvYuTyCeG","executionInfo":{"status":"ok","timestamp":1631394787554,"user_tz":420,"elapsed":25,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"d2590152-08de-4b08-8c0a-a95268a92a53"},"source":["cm = pd.DataFrame(confusion_matrix(labels[0],  preds[0]), columns = ['Normal', 'Abusive', 'Hate'])\n","cm.index = ['Normal', 'Abusive', 'Hate']\n","cm"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Normal</th>\n","      <th>Abusive</th>\n","      <th>Hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Normal</th>\n","      <td>195</td>\n","      <td>5</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>Abusive</th>\n","      <td>99</td>\n","      <td>101</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>Hate</th>\n","      <td>108</td>\n","      <td>92</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Normal  Abusive  Hate\n","Normal      195        5     0\n","Abusive      99      101     0\n","Hate        108       92     0"]},"metadata":{},"execution_count":29}]}]}