{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBerta_NN_scheduler_final","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"22d0cff70b9c4d999d53bc4cc27aa84f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6dd076cab5834f8ca3900d9f548ffb71","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_03480db734574d0f9f1c95f159744b5c","IPY_MODEL_d020fc6e796347418f972d6fec899365","IPY_MODEL_ecd362acc77d4796a6f624e7650a187f"]}},"6dd076cab5834f8ca3900d9f548ffb71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"03480db734574d0f9f1c95f159744b5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7df1c5ac5556434baba9db7cdd267933","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1d5199d024ad42999e3090415acdb7ef"}},"d020fc6e796347418f972d6fec899365":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_af75d502bc3d46a2b83007d907b38dda","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":898823,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":898823,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0c30ebc1f8ba4066ba0a2f4a358587b1"}},"ecd362acc77d4796a6f624e7650a187f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_39bdd876ac2a4c2389a9fbefdebadda1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 899k/899k [00:00&lt;00:00, 1.36MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_95696cac4d734574a2e01bf3ca7467f0"}},"7df1c5ac5556434baba9db7cdd267933":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1d5199d024ad42999e3090415acdb7ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"af75d502bc3d46a2b83007d907b38dda":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0c30ebc1f8ba4066ba0a2f4a358587b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"39bdd876ac2a4c2389a9fbefdebadda1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"95696cac4d734574a2e01bf3ca7467f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f60f085bbbba4ffcadd1d96cb77d7d98":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7111fa546cba4088bc0e60d91b8914cd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8dddf377e9554435a44f9e9cd2adacad","IPY_MODEL_990100bd85be4d988d9e5e505ebfc976","IPY_MODEL_add072a1e64f4054b4d35c690cb26fa9"]}},"7111fa546cba4088bc0e60d91b8914cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8dddf377e9554435a44f9e9cd2adacad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c71d60c3692749bcaf6913e7b0e48e7e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9853440f8dee4f7c9e3c9e95aba04b0a"}},"990100bd85be4d988d9e5e505ebfc976":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0f35483beb2e45f4b5b3d0372a72e229","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_be7546ec93ba44858943937495ad9cc9"}},"add072a1e64f4054b4d35c690cb26fa9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3ac38746ec934ad8ac6c30f0fdc98a8d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:00&lt;00:00, 1.10MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0f7928d2fc9e4492a41b6f0a9ad5c943"}},"c71d60c3692749bcaf6913e7b0e48e7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9853440f8dee4f7c9e3c9e95aba04b0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0f35483beb2e45f4b5b3d0372a72e229":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"be7546ec93ba44858943937495ad9cc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ac38746ec934ad8ac6c30f0fdc98a8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0f7928d2fc9e4492a41b6f0a9ad5c943":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ed829e2d2074d31af972793be111baa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_abb386ebaba549a690c8e94be975e0ff","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c6dbcef0fee4450da8b8652099a25d52","IPY_MODEL_73b4d91b2ea143588523762468998a1e","IPY_MODEL_22f1b7e69f2f44f8bd1664c1cd11c25b"]}},"abb386ebaba549a690c8e94be975e0ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c6dbcef0fee4450da8b8652099a25d52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_16102d54a2774418a4b8e154d8f916c4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_19c753c921244f119b8b5cfb56e92a79"}},"73b4d91b2ea143588523762468998a1e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_775cf6357c3546dcaa67263f45bbbdb3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1355863,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1355863,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b637afb56f24441ba791b6c556078f39"}},"22f1b7e69f2f44f8bd1664c1cd11c25b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_33bdd75399c546f6b4e20e9b3d7bdfc1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.36M/1.36M [00:00&lt;00:00, 5.36MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dd81b13be7eb480881427d6bae41a866"}},"16102d54a2774418a4b8e154d8f916c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"19c753c921244f119b8b5cfb56e92a79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"775cf6357c3546dcaa67263f45bbbdb3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b637afb56f24441ba791b6c556078f39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"33bdd75399c546f6b4e20e9b3d7bdfc1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dd81b13be7eb480881427d6bae41a866":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"28fb076c11cf452d97bc82e64fe5ce66":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f665b926b2064da9b64fe2a389764e8b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5be2926a22f04408b0067f38c07e8989","IPY_MODEL_66595ebaf9a442d1b8b4a1e4cd26ecb2","IPY_MODEL_faf4de39de1b4c30a6f3beb3636ab7fe"]}},"f665b926b2064da9b64fe2a389764e8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5be2926a22f04408b0067f38c07e8989":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e4e28f957ec94b169dcb4cd105bdcd7a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_26be6a33749d40f5a9cfe14188f93795"}},"66595ebaf9a442d1b8b4a1e4cd26ecb2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ecf9d82bb8284eaab6dfb7e20ee4482b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":481,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":481,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a3db5f7bb82e4f59a5045aff175240c5"}},"faf4de39de1b4c30a6f3beb3636ab7fe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_866d406ccd38457bba62a6f98c7a05cc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 481/481 [00:00&lt;00:00, 12.3kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2a73c6e30150463a8ce2b5e2e42e6814"}},"e4e28f957ec94b169dcb4cd105bdcd7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"26be6a33749d40f5a9cfe14188f93795":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ecf9d82bb8284eaab6dfb7e20ee4482b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a3db5f7bb82e4f59a5045aff175240c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"866d406ccd38457bba62a6f98c7a05cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2a73c6e30150463a8ce2b5e2e42e6814":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"555706b27db647a1959e7ce24917ec91":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2bb2a005d1fa49049a0a2360be654987","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b226f42d118e4b28a4ecd72a85c1edf9","IPY_MODEL_c901863a3d6446ff8298ee39f27de5b2","IPY_MODEL_6d9bcca091ad4fad8c483becd4183757"]}},"2bb2a005d1fa49049a0a2360be654987":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b226f42d118e4b28a4ecd72a85c1edf9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ed3cfb6feec84260b5f59356cd4b3d29","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f4784922eb024e51be66eed19c291adc"}},"c901863a3d6446ff8298ee39f27de5b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_944dc59104de42bf91dd910f58e4e619","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":501200538,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":501200538,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f96c44dd8d7b4c4ba764d9050aad9bd2"}},"6d9bcca091ad4fad8c483becd4183757":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9233721b824c451d958f5f62fda50148","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 501M/501M [00:16&lt;00:00, 29.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e2aac1d416af47dc9cb22c2afd68c100"}},"ed3cfb6feec84260b5f59356cd4b3d29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f4784922eb024e51be66eed19c291adc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"944dc59104de42bf91dd910f58e4e619":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f96c44dd8d7b4c4ba764d9050aad9bd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9233721b824c451d958f5f62fda50148":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e2aac1d416af47dc9cb22c2afd68c100":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pp3_2ZLq6NS","executionInfo":{"status":"ok","timestamp":1631143279882,"user_tz":420,"elapsed":15897,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"947d2565-9745-4ace-b843-2658e88c33d1"},"source":["from datetime import datetime\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import re, string\n","import time\n","import random\n","import gc\n","import os, sys\n","import json\n","from copy import deepcopy\n","\n","#####\n","# !pip install gdown\n","!pip install transformers\n","pd.options.display.max_colwidth = 500\n","#####\n","\n","import torch\n","\n","from transformers import BertForSequenceClassification, AdamW , BertConfig, BertModel\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import LineByLineTextDataset\n","\n","from torch.nn import LSTM, Dropout, Linear, BCELoss, CrossEntropyLoss, Softmax, ReLU, LeakyReLU, Tanh\n","from torch.nn.functional import softmax, relu\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.utils.data.dataset import Subset\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n","\n","%load_ext autoreload \n","%autoreload 2"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n","\u001b[K     |████████████████████████████████| 2.8 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 34.3 MB/s \n","\u001b[?25hCollecting huggingface-hub>=0.0.12\n","  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.4 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 48.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 28.1 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzEvkGACWMqX","executionInfo":{"status":"ok","timestamp":1631143303326,"user_tz":420,"elapsed":23457,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"231f00e2-c8fd-48e8-d396-24db2ce2fdda"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUYifLXWu1D0","executionInfo":{"status":"ok","timestamp":1631143303898,"user_tz":420,"elapsed":577,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"72454773-14a0-4b90-a2cf-931abbe33ed8"},"source":["!ls \"/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model\""],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["'Bert Models'   models.py   __pycache__   RoBerta  'Roberta Final'\n"]}]},{"cell_type":"code","metadata":{"id":"f6srHxsX4FhM","executionInfo":{"status":"ok","timestamp":1631143303898,"user_tz":420,"elapsed":3,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["PATH_PREFIX = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8keHP8s6Fj4","executionInfo":{"status":"ok","timestamp":1631143307463,"user_tz":420,"elapsed":3568,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils')\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model')\n","\n","from models import *\n","from text_processing import *"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFayi-JvrDHx","executionInfo":{"status":"ok","timestamp":1631143310581,"user_tz":420,"elapsed":3122,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"cec112a5-dceb-4a75-9bbf-9c574388115c"},"source":["# Enable GPU before proceeding\n","device_name = tf.test.gpu_device_name()\n","print(\"Device found:\", device_name)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Device found: /device:GPU:0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLkb-vZhryC-","executionInfo":{"status":"ok","timestamp":1631143310703,"user_tz":420,"elapsed":135,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"388f7f08-0ea9-44aa-b76e-99ea1a0ed2c1"},"source":["# Test if CUDA is available, use it\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","print(\"Using CUDA Device:\", torch.cuda.get_device_name())"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA Device: Tesla K80\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jgmig-gyT5_U"},"source":["# DOWNLOAD AND PREPROCESS DATA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIGMfizwRlrM","executionInfo":{"status":"ok","timestamp":1631143311273,"user_tz":420,"elapsed":572,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"aaf9fa39-5a9e-4c3c-9a00-074bac18abf5"},"source":["train_dt, test_dt = load_data(PATH_PREFIX + '/Data/HER_final_data.csv', \n","                   dtype_dict={'tweetid':str, 'label':int, 'target':int, 'aggr':int})"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["label\n","0    1894\n","1     304\n","2     202\n","Name: label, dtype: int64\n","target\n","0    2003\n","1     367\n","2      25\n","3       5\n","Name: target, dtype: int64\n","aggr\n","0    1799\n","1     533\n","2      68\n","Name: aggr, dtype: int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"42nIFpyRNTJw"},"source":["## Tokenize state"]},{"cell_type":"code","metadata":{"id":"GPIccBVB_yr1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631143311497,"user_tz":420,"elapsed":226,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"0e6a9cc8-8a39-40f0-9cfe-4ac78cf20256"},"source":["# import hashtag_dict \n","HASHTAG_DIR = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils/' \n","with open(HASHTAG_DIR + 'hashtag_dict.json', 'r') as f:\n","    hashtag_dict = json.load(f)\n","\n","# CUDA is memory hungry. Needs to reduce the max_len\n","MAX_LEN = 225\n","print(\"Maximum length of cleaned up dataset is:\", MAX_LEN)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum length of cleaned up dataset is: 225\n"]}]},{"cell_type":"code","metadata":{"id":"CIPViSOuXhTA","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["22d0cff70b9c4d999d53bc4cc27aa84f","6dd076cab5834f8ca3900d9f548ffb71","03480db734574d0f9f1c95f159744b5c","d020fc6e796347418f972d6fec899365","ecd362acc77d4796a6f624e7650a187f","7df1c5ac5556434baba9db7cdd267933","1d5199d024ad42999e3090415acdb7ef","af75d502bc3d46a2b83007d907b38dda","0c30ebc1f8ba4066ba0a2f4a358587b1","39bdd876ac2a4c2389a9fbefdebadda1","95696cac4d734574a2e01bf3ca7467f0","f60f085bbbba4ffcadd1d96cb77d7d98","7111fa546cba4088bc0e60d91b8914cd","8dddf377e9554435a44f9e9cd2adacad","990100bd85be4d988d9e5e505ebfc976","add072a1e64f4054b4d35c690cb26fa9","c71d60c3692749bcaf6913e7b0e48e7e","9853440f8dee4f7c9e3c9e95aba04b0a","0f35483beb2e45f4b5b3d0372a72e229","be7546ec93ba44858943937495ad9cc9","3ac38746ec934ad8ac6c30f0fdc98a8d","0f7928d2fc9e4492a41b6f0a9ad5c943","3ed829e2d2074d31af972793be111baa","abb386ebaba549a690c8e94be975e0ff","c6dbcef0fee4450da8b8652099a25d52","73b4d91b2ea143588523762468998a1e","22f1b7e69f2f44f8bd1664c1cd11c25b","16102d54a2774418a4b8e154d8f916c4","19c753c921244f119b8b5cfb56e92a79","775cf6357c3546dcaa67263f45bbbdb3","b637afb56f24441ba791b6c556078f39","33bdd75399c546f6b4e20e9b3d7bdfc1","dd81b13be7eb480881427d6bae41a866","28fb076c11cf452d97bc82e64fe5ce66","f665b926b2064da9b64fe2a389764e8b","5be2926a22f04408b0067f38c07e8989","66595ebaf9a442d1b8b4a1e4cd26ecb2","faf4de39de1b4c30a6f3beb3636ab7fe","e4e28f957ec94b169dcb4cd105bdcd7a","26be6a33749d40f5a9cfe14188f93795","ecf9d82bb8284eaab6dfb7e20ee4482b","a3db5f7bb82e4f59a5045aff175240c5","866d406ccd38457bba62a6f98c7a05cc","2a73c6e30150463a8ce2b5e2e42e6814"]},"executionInfo":{"status":"ok","timestamp":1631143314063,"user_tz":420,"elapsed":2568,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"a519d08a-806d-46f4-c67a-1d943a3217f3"},"source":["tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22d0cff70b9c4d999d53bc4cc27aa84f","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f60f085bbbba4ffcadd1d96cb77d7d98","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ed829e2d2074d31af972793be111baa","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28fb076c11cf452d97bc82e64fe5ce66","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HP_qSMkuohrO","executionInfo":{"status":"ok","timestamp":1631143316131,"user_tz":420,"elapsed":2072,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"82f6d2e6-1c82-4448-d08d-47cef3ce28d2"},"source":["train_input_ids, train_attn_masks  = tokenizing(train_dt.text.values, tokenizer, MAX_LEN)\n","test_input_ids, test_attn_masks  = tokenizing(test_dt.text.values, tokenizer, MAX_LEN)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Total time taken:  1.6630587577819824\n","Total time taken:  0.36176061630249023\n"]}]},{"cell_type":"code","metadata":{"id":"5UaN8FNEUbn9","executionInfo":{"status":"ok","timestamp":1631143316357,"user_tz":420,"elapsed":232,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["varlist = ['label', 'target', 'aggr']\n","_, train_tensor = arrange_tensor(train_dt, train_input_ids, train_attn_masks ,varlist=varlist)\n","_, test_tensor  = arrange_tensor(test_dt, test_input_ids, test_attn_masks ,varlist=varlist)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RNJyuEsVfCB","executionInfo":{"status":"ok","timestamp":1631143316361,"user_tz":420,"elapsed":7,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"e908fd4a-52ee-4e83-f526-bdd10e09bed8"},"source":["test_tensor[:][-1].shape[0]"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["600"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"jKEuQnyKzKaP"},"source":["# BUILD MODEL"]},{"cell_type":"markdown","metadata":{"id":"ptpLoOHEKWBx"},"source":["In this model, we will build an LSTM model leveraging BERT base hidden data on top. The key difference compared to simpleBert is that we need to save the parameters."]},{"cell_type":"code","metadata":{"id":"t0NtCmP8NM8B","executionInfo":{"status":"ok","timestamp":1631143316499,"user_tz":420,"elapsed":143,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Saved Parameters/\"\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"2K9sRD_qgCEY","executionInfo":{"status":"ok","timestamp":1631143316590,"user_tz":420,"elapsed":94,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["def run_models(save_path:str, model_class, num_labels, num_tasks, max_len,\n","        num_epochs, loss_funcs:list, device,\n","        train_tensor, test_tensor, val_tensor=None, batch_size=20, label_ls = [[0,1,2]],\n","        seed_list = [101,211,307,401,503], save_seed=0, \n","        scheduler=get_linear_schedule_with_warmup):\n","\n","    final_metrics = dict()\n","    # Iterate through seeds\n","    for idx, s in enumerate(seed_list):\n","        print('========== Trying seed', s, '==========')\n","        # Reset the seed\n","        seed = s\n","        random.seed(seed)\n","        os.environ['PYTHONHASHSEED'] = str(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = True\n","    \n","        # print(\"Random num: \", random.randint(1,1000))\n","        \n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        # Initialize model\n","        model = model_class('roberta-base', num_labels=num_labels, max_len=max_len)\n","        optimizer = AdamW(model.parameters(),lr=2e-5, eps=1e-6, weight_decay=1e-3)\n","        build_params = {'device':device, 'optimizer': optimizer, 'loss_functions': loss_funcs}\n","        # build \n","        model.build(**build_params)\n","\n","        # renew scheduler\n","        total_steps = train_tensor[:][-1].shape[0] * num_epochs\n","        scheduler_new = scheduler(model.opt, \n","                                  num_warmup_steps = 0, \n","                                  num_training_steps = total_steps)\n","        # train\n","        if val_tensor is None:\n","            model.train(train_tensor, batch_size = batch_size,\n","                        num_epochs=num_epochs, verbose=True, scheduler = scheduler_new)\n","        else: \n","            model.train(train_tensor, val_tensor=val_tensor, batch_size=batch_size,\n","                        num_epochs=num_epochs, verbose=True, scheduler = scheduler_new)\n","\n","        # evaluate\n","        _, logits, labels = model.evaluate(test_tensor)\n","        logits, labels = format_outputs(logits, labels, num_tasks)\n","        preds = get_labels_from_multi_logits(logits)\n","        res = print_report_multitask(labels, preds, label_ls )\n","\n","        final_metrics[s] = res\n","\n","        if idx==save_seed:\n","            model.save(save_path)\n","        del model\n","\n","\n","    return final_metrics "],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTFb-MmerbQ6"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["555706b27db647a1959e7ce24917ec91","2bb2a005d1fa49049a0a2360be654987","b226f42d118e4b28a4ecd72a85c1edf9","c901863a3d6446ff8298ee39f27de5b2","6d9bcca091ad4fad8c483becd4183757","ed3cfb6feec84260b5f59356cd4b3d29","f4784922eb024e51be66eed19c291adc","944dc59104de42bf91dd910f58e4e619","f96c44dd8d7b4c4ba764d9050aad9bd2","9233721b824c451d958f5f62fda50148","e2aac1d416af47dc9cb22c2afd68c100"]},"id":"_iTm811coQuf","executionInfo":{"status":"ok","timestamp":1631147979242,"user_tz":420,"elapsed":4662654,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"5219cb32-7ef9-4f19-a7ef-a19bc2934158"},"source":["###### !!CHECK THIS!! ######\n","model_class = RobertaNN\n","model_str = 'Roberta_NN'\n","###########################\n","target_ls = ['label','target','aggr']\n","num_labels =  [3,4,3]\n","num_tasks = len(num_labels)\n","label_ls = [[0,1,2],[0,1,2,3],[0,1,2]]\n","num_epochs = 5\n","# change correct weights\n","weight_dict = calculate_class_weights(train_dt, target_ls)\n","weight_ls  = [list(weight_dict[k].values()) for k in target_ls]\n","loss_funcs = [CrossEntropyLoss(weight=torch.tensor(w).to(device)) for w in weight_ls]\n","\n","model_params = {'num_labels':num_labels, 'num_tasks':num_tasks, \n","                'max_len':MAX_LEN,'label_ls':label_ls,\n","                'num_epochs':num_epochs, 'loss_funcs':loss_funcs,\n","                }\n","\n","results = run_models(save_path = SAVE_PATH, model_class=model_class, device=device, \n","     train_tensor=train_tensor, test_tensor=test_tensor, \n","    #  seed_list=[101,211],\n","      **model_params )"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["========== Trying seed 101 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"555706b27db647a1959e7ce24917ec91","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [117.75027740001678, 122.51491105556488, 121.6820536851883]. Validation loss: []. Duration: 181.85987997055054\n","Running Epoch  1\n","Training loss : [90.07498753070831, 80.29932929575443, 88.7134955227375]. Validation loss: []. Duration: 181.11104893684387\n","Running Epoch  2\n","Training loss : [80.79996101558208, 63.48216009512544, 72.52905201911926]. Validation loss: []. Duration: 181.24537706375122\n","Running Epoch  3\n","Training loss : [74.64183436334133, 54.867318507283926, 65.70541068166494]. Validation loss: []. Duration: 181.24620866775513\n","Running Epoch  4\n","Training loss : [66.71150070428848, 37.889342268928885, 47.75735408067703]. Validation loss: []. Duration: 181.19369053840637\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.93191489 0.484375   0.34848485]\n","recall   : [0.90309278 0.3974359  0.62162162]\n","fscore   : [0.91727749 0.43661972 0.44660194]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.94616977 0.62264151 0.18181818 0.        ]\n","recall   : [0.91035857 0.75862069 0.2        0.        ]\n","fscore   : [0.92791878 0.68393782 0.19047619 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.89726027 0.6        0.5       ]\n","recall   : [0.88914027 0.65217391 0.3       ]\n","fscore   : [0.89318182 0.625      0.375     ]\n","support  : [442 138  20]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["========== Trying seed 211 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [115.67605739831924, 116.8986789137125, 117.881562769413]. Validation loss: []. Duration: 181.66642999649048\n","Running Epoch  1\n","Training loss : [91.47385719418526, 81.55292209982872, 87.49368572235107]. Validation loss: []. Duration: 181.33694553375244\n","Running Epoch  2\n","Training loss : [78.27859203517437, 59.09551791474223, 66.7560783624649]. Validation loss: []. Duration: 181.50195264816284\n","Running Epoch  3\n","Training loss : [73.38043908774853, 50.1165293417871, 52.91484870016575]. Validation loss: []. Duration: 181.46902775764465\n","Running Epoch  4\n","Training loss : [65.8629005625844, 38.05632552690804, 44.361581303179264]. Validation loss: []. Duration: 181.39415764808655\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.95673077 0.34939759 0.2970297 ]\n","recall   : [0.82061856 0.37179487 0.81081081]\n","fscore   : [0.88346282 0.36024845 0.43478261]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.97477064 0.61068702 0.12903226 0.        ]\n","recall   : [0.84661355 0.91954023 0.4        0.        ]\n","fscore   : [0.90618337 0.73394495 0.19512195 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.93059126 0.54271357 0.25      ]\n","recall   : [0.81900452 0.7826087  0.15      ]\n","fscore   : [0.87123947 0.64094955 0.1875    ]\n","support  : [442 138  20]\n","========== Trying seed 307 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [118.85624668002129, 130.71809968352318, 117.83578318357468]. Validation loss: []. Duration: 181.34614157676697\n","Running Epoch  1\n","Training loss : [89.43170127272606, 86.27417661249638, 85.48583823442459]. Validation loss: []. Duration: 181.3567831516266\n","Running Epoch  2\n","Training loss : [78.5953967422247, 61.25464163720608, 65.79629762470722]. Validation loss: []. Duration: 181.3183159828186\n","Running Epoch  3\n","Training loss : [76.35309909284115, 54.63128113374114, 63.406781040132046]. Validation loss: []. Duration: 181.273268699646\n","Running Epoch  4\n","Training loss : [69.0954263061285, 41.83093253523111, 52.70921865105629]. Validation loss: []. Duration: 181.36346125602722\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.93932584 0.4        0.28235294]\n","recall   : [0.86185567 0.35897436 0.64864865]\n","fscore   : [0.89892473 0.37837838 0.39344262]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.94503171 0.63207547 0.1        0.        ]\n","recall   : [0.89043825 0.77011494 0.2        0.        ]\n","fscore   : [0.91692308 0.69430052 0.13333333 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.90675991 0.58895706 0.5       ]\n","recall   : [0.8800905  0.69565217 0.2       ]\n","fscore   : [0.89322618 0.63787375 0.28571429]\n","support  : [442 138  20]\n","========== Trying seed 401 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [115.22898072004318, 124.56934669613838, 114.2913191318512]. Validation loss: []. Duration: 181.24752116203308\n","Running Epoch  1\n","Training loss : [89.16564479470253, 82.3153698593378, 83.14504684507847]. Validation loss: []. Duration: 181.32026839256287\n","Running Epoch  2\n","Training loss : [80.95903342962265, 63.01706852763891, 68.90433299541473]. Validation loss: []. Duration: 181.326828956604\n","Running Epoch  3\n","Training loss : [72.93872018158436, 53.412820391356945, 57.011789076030254]. Validation loss: []. Duration: 181.19436883926392\n","Running Epoch  4\n","Training loss : [70.49440594017506, 44.552617916837335, 50.707200376316905]. Validation loss: []. Duration: 181.32109260559082\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.95022624 0.41025641 0.3125    ]\n","recall   : [0.86597938 0.41025641 0.67567568]\n","fscore   : [0.90614887 0.41025641 0.42735043]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.96853933 0.62608696 0.15384615 0.        ]\n","recall   : [0.85856574 0.82758621 0.6        0.        ]\n","fscore   : [0.91024287 0.71287129 0.24489796 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.91442543 0.56216216 0.5       ]\n","recall   : [0.84615385 0.75362319 0.15      ]\n","fscore   : [0.87896592 0.64396285 0.23076923]\n","support  : [442 138  20]\n","========== Trying seed 503 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [116.29779931902885, 126.74136345088482, 115.77709224820137]. Validation loss: []. Duration: 181.3223443031311\n","Running Epoch  1\n","Training loss : [93.09509786963463, 88.85159280896187, 85.16164070367813]. Validation loss: []. Duration: 181.24561834335327\n","Running Epoch  2\n","Training loss : [77.77742514014244, 65.93147614598274, 64.85392060875893]. Validation loss: []. Duration: 181.13025903701782\n","Running Epoch  3\n","Training loss : [74.70187942683697, 54.61283937841654, 56.029889293015]. Validation loss: []. Duration: 181.22939085960388\n","Running Epoch  4\n","Training loss : [68.31535057723522, 43.931325133889914, 47.0344325453043]. Validation loss: []. Duration: 181.36284732818604\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.91880342 0.39622642 0.3164557 ]\n","recall   : [0.88659794 0.26923077 0.67567568]\n","fscore   : [0.90241343 0.32061069 0.43103448]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.95493562 0.61344538 0.13333333 0.        ]\n","recall   : [0.88645418 0.83908046 0.2        0.        ]\n","fscore   : [0.91942149 0.70873786 0.16       0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.87391304 0.60769231 0.6       ]\n","recall   : [0.90950226 0.57246377 0.3       ]\n","fscore   : [0.89135255 0.58955224 0.4       ]\n","support  : [442 138  20]\n"]}]},{"cell_type":"code","metadata":{"id":"HPpGBDuFRMcl","executionInfo":{"status":"ok","timestamp":1631147979395,"user_tz":420,"elapsed":165,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":["RESULT_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Results/\"\n","\n","specific_res = get_specific_results(results)\n","specific_res.to_pickle(RESULT_PATH + model_str + '_specific.pkl')\n","\n","macro_res    = get_macro_results(results)\n","macro_res.to_pickle(RESULT_PATH + model_str + '_macro.pkl')"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3V4slhXyCB_","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1631147979491,"user_tz":420,"elapsed":97,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"3b35602f-61f3-4e01-becc-6d4a5590fec1"},"source":["specific_res"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Normal</th>\n","      <th>Abusive</th>\n","      <th>Hate</th>\n","      <th>Neither/NA</th>\n","      <th>Anti-Asian</th>\n","      <th>Anti-Black</th>\n","      <th>Both</th>\n","      <th>No.Agg.</th>\n","      <th>Some.Agg.</th>\n","      <th>Very.Agg.</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.94 +- 0.01</td>\n","      <td>0.41 +- 0.04</td>\n","      <td>0.31 +- 0.02</td>\n","      <td>0.96 +- 0.01</td>\n","      <td>0.62 +- 0.01</td>\n","      <td>0.14 +- 0.03</td>\n","      <td>--</td>\n","      <td>0.9 +- 0.02</td>\n","      <td>0.58 +- 0.02</td>\n","      <td>0.47 +- 0.12</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.87 +- 0.03</td>\n","      <td>0.36 +- 0.05</td>\n","      <td>0.69 +- 0.07</td>\n","      <td>0.88 +- 0.02</td>\n","      <td>0.82 +- 0.06</td>\n","      <td>0.32 +- 0.16</td>\n","      <td>--</td>\n","      <td>0.87 +- 0.03</td>\n","      <td>0.69 +- 0.07</td>\n","      <td>0.22 +- 0.07</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.9 +- 0.01</td>\n","      <td>0.38 +- 0.04</td>\n","      <td>0.43 +- 0.02</td>\n","      <td>0.92 +- 0.01</td>\n","      <td>0.71 +- 0.02</td>\n","      <td>0.18 +- 0.04</td>\n","      <td>--</td>\n","      <td>0.89 +- 0.01</td>\n","      <td>0.63 +- 0.02</td>\n","      <td>0.3 +- 0.08</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Normal       Abusive  ...     Some.Agg.     Very.Agg.\n","0  0.94 +- 0.01  0.41 +- 0.04  ...  0.58 +- 0.02  0.47 +- 0.12\n","1  0.87 +- 0.03  0.36 +- 0.05  ...  0.69 +- 0.07  0.22 +- 0.07\n","2   0.9 +- 0.01  0.38 +- 0.04  ...  0.63 +- 0.02   0.3 +- 0.08\n","\n","[3 rows x 10 columns]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"6ystxKXvLkG4","executionInfo":{"status":"ok","timestamp":1631147979583,"user_tz":420,"elapsed":95,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"94e45819-0b5d-45d1-fd53-aad605931882"},"source":["macro_res"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.55 +- 0.02</td>\n","      <td>0.64 +- 0.02</td>\n","      <td>0.59 +- 0.01</td>\n","      <td>0.79 +- 0.01</td>\n","      <td>0.85 +- 0.02</td>\n","      <td>0.82 +- 0.01</td>\n","      <td>0.65 +- 0.04</td>\n","      <td>0.59 +- 0.01</td>\n","      <td>0.62 +- 0.02</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              0             1  ...             7             8\n","0  0.55 +- 0.02  0.64 +- 0.02  ...  0.59 +- 0.01  0.62 +- 0.02\n","\n","[1 rows x 9 columns]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"leJMgoirLl1_","executionInfo":{"status":"ok","timestamp":1631147979583,"user_tz":420,"elapsed":4,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}}},"source":[""],"execution_count":19,"outputs":[]}]}