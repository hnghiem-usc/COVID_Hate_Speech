{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBerta_LSTM_final","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1c20b236ff094fa18e956fd4adcce05b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7b65be90411c4127a7602967e4bb0c94","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_731a05be2d9041ef8854a23648b8c981","IPY_MODEL_4474ddc3f9d64401a19969f5a16c3553","IPY_MODEL_302b40b692c04e029226eb980a9f601c"]}},"7b65be90411c4127a7602967e4bb0c94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"731a05be2d9041ef8854a23648b8c981":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e7d92ad68f424639af38c1454f41b05d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_36410273cb3c403a831a6c4014463931"}},"4474ddc3f9d64401a19969f5a16c3553":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_370230060e5444a8810d63511b267181","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":898823,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":898823,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b5a63b7b659f43c3ba74667282c5eb72"}},"302b40b692c04e029226eb980a9f601c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b35255c4a8d1418aa0ea915e7afcbdaf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 899k/899k [00:00&lt;00:00, 1.29MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f3f9ca4386ec4c75b2998db89480e19c"}},"e7d92ad68f424639af38c1454f41b05d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"36410273cb3c403a831a6c4014463931":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"370230060e5444a8810d63511b267181":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b5a63b7b659f43c3ba74667282c5eb72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b35255c4a8d1418aa0ea915e7afcbdaf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f3f9ca4386ec4c75b2998db89480e19c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f2b0e3ad46d14c5d920d0a63d070f540":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e42d904df8644dbc8a4796ed9db4eefd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e6f65e818233468f99dfee20afb1e8f0","IPY_MODEL_8dde6fcce3b04f41bf0c31e7352c3c05","IPY_MODEL_3afe7ac380cd4ad6807cb5fc89add39d"]}},"e42d904df8644dbc8a4796ed9db4eefd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e6f65e818233468f99dfee20afb1e8f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7ed5b431293149fd80dc4909298b2953","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2c2e7bc464f349beb5410da57155cb81"}},"8dde6fcce3b04f41bf0c31e7352c3c05":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e1bebdef01b64e3eafc4cf8398801c6e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_620a2b5a605f46e9b4d2bc4d92b89f39"}},"3afe7ac380cd4ad6807cb5fc89add39d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e0bde825cf0245e7b467dfacf7ccb173","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:00&lt;00:00, 1.01MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8799d577bf234737a80af38ed3dfe56c"}},"7ed5b431293149fd80dc4909298b2953":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2c2e7bc464f349beb5410da57155cb81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e1bebdef01b64e3eafc4cf8398801c6e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"620a2b5a605f46e9b4d2bc4d92b89f39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0bde825cf0245e7b467dfacf7ccb173":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8799d577bf234737a80af38ed3dfe56c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bead2137fb3b44fe8f48f0f587d91494":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c9096066fd2a49f7a87fa0a921574d9c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f1d495f17d874a659b44ce2267a7a582","IPY_MODEL_c0fabbcf7ef94ec8bf4caa450928d8ee","IPY_MODEL_659ce96c8e7141c7984efd279912d610"]}},"c9096066fd2a49f7a87fa0a921574d9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f1d495f17d874a659b44ce2267a7a582":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5b2c85093662442782314691ece8c858","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_30fd40692087411488c8aa013b266c2b"}},"c0fabbcf7ef94ec8bf4caa450928d8ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fc803202d78a4a7082d961ff5d2ef455","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1355863,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1355863,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d2f3702a77a74ec58f2b267251e1c57e"}},"659ce96c8e7141c7984efd279912d610":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9cec3615738049438c7b4232c51c85f9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.36M/1.36M [00:00&lt;00:00, 7.57MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6b6c108fc5e348a2b5e28797af65b904"}},"5b2c85093662442782314691ece8c858":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"30fd40692087411488c8aa013b266c2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fc803202d78a4a7082d961ff5d2ef455":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d2f3702a77a74ec58f2b267251e1c57e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9cec3615738049438c7b4232c51c85f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6b6c108fc5e348a2b5e28797af65b904":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3bd34bd08c2b4f82823525012c9b7851":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2a1b0be338554cf1bf53d3c9e26a0eed","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6181804da4af4cc8a3dc3e799b53f438","IPY_MODEL_7b59a36821f341079d1a302b4baadf91","IPY_MODEL_f0f5d15bea124481a25d9ea90b6f1fd3"]}},"2a1b0be338554cf1bf53d3c9e26a0eed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6181804da4af4cc8a3dc3e799b53f438":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b2eb2e2fcee1407589e9be7bb927d1aa","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bce4b0edd83947e098392ddc17ae6b41"}},"7b59a36821f341079d1a302b4baadf91":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3d27392373184472930f3ee9ec29db88","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":481,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":481,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_85ebfbb1223649d48dab4664c19fd400"}},"f0f5d15bea124481a25d9ea90b6f1fd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_17d679529d3d4fefb983da69c83aa06f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 481/481 [00:00&lt;00:00, 9.86kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c8d14e908099429093cae73352abcf4c"}},"b2eb2e2fcee1407589e9be7bb927d1aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"bce4b0edd83947e098392ddc17ae6b41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3d27392373184472930f3ee9ec29db88":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"85ebfbb1223649d48dab4664c19fd400":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"17d679529d3d4fefb983da69c83aa06f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c8d14e908099429093cae73352abcf4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ef1951dce38e41efa2a8a7a24d229369":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8e1c4cb79b1d4499b0232d2957e67fd8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_362c9e0467ce4e9291bcac784f627e18","IPY_MODEL_b4fc60761e0d46239eacb75f221ed464","IPY_MODEL_c2a6d35544a149eebcf1590a1158b07e"]}},"8e1c4cb79b1d4499b0232d2957e67fd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"362c9e0467ce4e9291bcac784f627e18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f6a16236703a4556bcd08f420b57efe8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_11211c9126e64ab8a1213c69b5d7e7e0"}},"b4fc60761e0d46239eacb75f221ed464":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cad94fd0b4c745c3932d2aaffb26cba6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":501200538,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":501200538,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d399b44d705b4da19a33ed4ea31c14ba"}},"c2a6d35544a149eebcf1590a1158b07e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2d11ee017ffe4cb5bace8e78b4321da7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 501M/501M [00:17&lt;00:00, 31.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_81f1cbb317ac4773851da40c833d125d"}},"f6a16236703a4556bcd08f420b57efe8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"11211c9126e64ab8a1213c69b5d7e7e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cad94fd0b4c745c3932d2aaffb26cba6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d399b44d705b4da19a33ed4ea31c14ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2d11ee017ffe4cb5bace8e78b4321da7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"81f1cbb317ac4773851da40c833d125d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pp3_2ZLq6NS","executionInfo":{"status":"ok","timestamp":1631075093863,"user_tz":420,"elapsed":15563,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"e458d907-7594-42ed-d586-7a938fd7716c"},"source":["from datetime import datetime\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import re, string\n","import time\n","import random\n","import gc\n","import os, sys\n","import json\n","from copy import deepcopy\n","\n","#####\n","# !pip install gdown\n","!pip install transformers\n","pd.options.display.max_colwidth = 500\n","#####\n","\n","import torch\n","\n","from transformers import BertForSequenceClassification, AdamW , BertConfig, BertModel\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import LineByLineTextDataset\n","\n","from torch.nn import LSTM, Dropout, Linear, BCELoss, CrossEntropyLoss, Softmax, ReLU, LeakyReLU, Tanh\n","from torch.nn.functional import softmax, relu\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.utils.data.dataset import Subset\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n","\n","%load_ext autoreload \n","%autoreload 2"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n","\u001b[K     |████████████████████████████████| 2.8 MB 5.2 MB/s \n","\u001b[?25hCollecting huggingface-hub>=0.0.12\n","  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 48.6 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 52.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 37.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzEvkGACWMqX","executionInfo":{"status":"ok","timestamp":1631075155169,"user_tz":420,"elapsed":61311,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"4604bdd3-5687-4778-8919-0b77415cd3d5"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUYifLXWu1D0","executionInfo":{"status":"ok","timestamp":1631075155765,"user_tz":420,"elapsed":600,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"c6fd7915-c61a-4fe6-f9af-ecb8a6da5916"},"source":["!ls \"/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model\""],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["'Bert Models'   models.py   __pycache__   RoBerta  'Roberta Final'\n"]}]},{"cell_type":"code","metadata":{"id":"f6srHxsX4FhM","executionInfo":{"status":"ok","timestamp":1631075155894,"user_tz":420,"elapsed":132,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":["PATH_PREFIX = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8keHP8s6Fj4","executionInfo":{"status":"ok","timestamp":1631075158734,"user_tz":420,"elapsed":2842,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":["sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils')\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model')\n","\n","from models import *\n","from text_processing import *"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFayi-JvrDHx","executionInfo":{"status":"ok","timestamp":1631075161512,"user_tz":420,"elapsed":2781,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"8b6f5527-fb78-4360-8fb5-bf56192dda07"},"source":["# Enable GPU before proceeding\n","device_name = tf.test.gpu_device_name()\n","print(\"Device found:\", device_name)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Device found: /device:GPU:0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLkb-vZhryC-","executionInfo":{"status":"ok","timestamp":1631075161665,"user_tz":420,"elapsed":157,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"dd2692b5-3699-46b1-dd48-d896ebffe250"},"source":["# Test if CUDA is available, use it\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","print(\"Using CUDA Device:\", torch.cuda.get_device_name())"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA Device: Tesla K80\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jgmig-gyT5_U"},"source":["# DOWNLOAD AND PREPROCESS DATA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIGMfizwRlrM","executionInfo":{"status":"ok","timestamp":1631075162359,"user_tz":420,"elapsed":696,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"d16b9dbb-3e15-4a06-f2d4-3cc2cff89565"},"source":["train_dt, test_dt = load_data(PATH_PREFIX + '/Data/HER_final_data.csv', \n","                   dtype_dict={'tweetid':str, 'label':int, 'target':int, 'aggr':int})"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["label\n","0    1894\n","1     304\n","2     202\n","Name: label, dtype: int64\n","target\n","0    2003\n","1     367\n","2      25\n","3       5\n","Name: target, dtype: int64\n","aggr\n","0    1799\n","1     533\n","2      68\n","Name: aggr, dtype: int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"42nIFpyRNTJw"},"source":["## Tokenize state"]},{"cell_type":"code","metadata":{"id":"GPIccBVB_yr1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631075162639,"user_tz":420,"elapsed":283,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"ec28ffc4-2c71-4a26-b45b-9335479dc639"},"source":["# import hashtag_dict \n","HASHTAG_DIR = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils/' \n","with open(HASHTAG_DIR + 'hashtag_dict.json', 'r') as f:\n","    hashtag_dict = json.load(f)\n","\n","# CUDA is memory hungry. Needs to reduce the max_len\n","MAX_LEN = 225\n","print(\"Maximum length of cleaned up dataset is:\", MAX_LEN)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum length of cleaned up dataset is: 225\n"]}]},{"cell_type":"code","metadata":{"id":"CIPViSOuXhTA","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["1c20b236ff094fa18e956fd4adcce05b","7b65be90411c4127a7602967e4bb0c94","731a05be2d9041ef8854a23648b8c981","4474ddc3f9d64401a19969f5a16c3553","302b40b692c04e029226eb980a9f601c","e7d92ad68f424639af38c1454f41b05d","36410273cb3c403a831a6c4014463931","370230060e5444a8810d63511b267181","b5a63b7b659f43c3ba74667282c5eb72","b35255c4a8d1418aa0ea915e7afcbdaf","f3f9ca4386ec4c75b2998db89480e19c","f2b0e3ad46d14c5d920d0a63d070f540","e42d904df8644dbc8a4796ed9db4eefd","e6f65e818233468f99dfee20afb1e8f0","8dde6fcce3b04f41bf0c31e7352c3c05","3afe7ac380cd4ad6807cb5fc89add39d","7ed5b431293149fd80dc4909298b2953","2c2e7bc464f349beb5410da57155cb81","e1bebdef01b64e3eafc4cf8398801c6e","620a2b5a605f46e9b4d2bc4d92b89f39","e0bde825cf0245e7b467dfacf7ccb173","8799d577bf234737a80af38ed3dfe56c","bead2137fb3b44fe8f48f0f587d91494","c9096066fd2a49f7a87fa0a921574d9c","f1d495f17d874a659b44ce2267a7a582","c0fabbcf7ef94ec8bf4caa450928d8ee","659ce96c8e7141c7984efd279912d610","5b2c85093662442782314691ece8c858","30fd40692087411488c8aa013b266c2b","fc803202d78a4a7082d961ff5d2ef455","d2f3702a77a74ec58f2b267251e1c57e","9cec3615738049438c7b4232c51c85f9","6b6c108fc5e348a2b5e28797af65b904","3bd34bd08c2b4f82823525012c9b7851","2a1b0be338554cf1bf53d3c9e26a0eed","6181804da4af4cc8a3dc3e799b53f438","7b59a36821f341079d1a302b4baadf91","f0f5d15bea124481a25d9ea90b6f1fd3","b2eb2e2fcee1407589e9be7bb927d1aa","bce4b0edd83947e098392ddc17ae6b41","3d27392373184472930f3ee9ec29db88","85ebfbb1223649d48dab4664c19fd400","17d679529d3d4fefb983da69c83aa06f","c8d14e908099429093cae73352abcf4c"]},"executionInfo":{"status":"ok","timestamp":1631075165212,"user_tz":420,"elapsed":2576,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"a60a770c-dafe-433b-db68-884006593c49"},"source":["tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c20b236ff094fa18e956fd4adcce05b","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2b0e3ad46d14c5d920d0a63d070f540","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bead2137fb3b44fe8f48f0f587d91494","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bd34bd08c2b4f82823525012c9b7851","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HP_qSMkuohrO","executionInfo":{"status":"ok","timestamp":1631075167267,"user_tz":420,"elapsed":2056,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"7ece2f65-6254-42b3-f5e5-6e0150d9ec51"},"source":["train_input_ids, train_attn_masks  = tokenizing(train_dt.text.values, tokenizer, MAX_LEN)\n","test_input_ids, test_attn_masks  = tokenizing(test_dt.text.values, tokenizer, MAX_LEN)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Total time taken:  1.6608128547668457\n","Total time taken:  0.3696784973144531\n"]}]},{"cell_type":"code","metadata":{"id":"5UaN8FNEUbn9","executionInfo":{"status":"ok","timestamp":1631075167392,"user_tz":420,"elapsed":128,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":["varlist = ['label', 'target', 'aggr']\n","_, train_tensor = arrange_tensor(train_dt, train_input_ids, train_attn_masks ,varlist=varlist)\n","_, test_tensor  = arrange_tensor(test_dt, test_input_ids, test_attn_masks ,varlist=varlist)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RNJyuEsVfCB","executionInfo":{"status":"ok","timestamp":1631075167493,"user_tz":420,"elapsed":103,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"98c010cf-2b65-4084-dd23-106c6d21add6"},"source":["test_tensor[:][-1].shape"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([600])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"jKEuQnyKzKaP"},"source":["# BUILD MODEL"]},{"cell_type":"markdown","metadata":{"id":"ptpLoOHEKWBx"},"source":["In this model, we will build an LSTM model leveraging BERT base hidden data on top. The key difference compared to simpleBert is that we need to save the parameters."]},{"cell_type":"code","metadata":{"id":"t0NtCmP8NM8B","executionInfo":{"status":"ok","timestamp":1631075167606,"user_tz":420,"elapsed":114,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":["SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Saved Parameters/\""],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"2K9sRD_qgCEY","executionInfo":{"status":"ok","timestamp":1631075167719,"user_tz":420,"elapsed":115,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":["def run_models(save_path:str, model_class, num_labels, num_tasks, max_len,\n","        num_epochs, loss_funcs:list, device,\n","        train_tensor, test_tensor, val_tensor=None, batch_size=20, label_ls = [[0,1,2]],\n","        seed_list = [101,211,307,401,503], save_seed=0):\n","\n","    final_metrics = dict()\n","    # Iterate through seeds\n","    for idx, s in enumerate(seed_list):\n","        print('========== Trying seed', s, '==========')\n","        # Reset the seed\n","        seed = s\n","        random.seed(seed)\n","        os.environ['PYTHONHASHSEED'] = str(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = True\n","    \n","        # print(\"Random num: \", random.randint(1,1000))\n","        \n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        # Initialize model\n","        model = model_class('roberta-base', num_labels=num_labels, max_len=max_len)\n","        optimizer = AdamW(model.parameters(),lr=2e-5, eps=1e-6, weight_decay=1e-3)\n","        build_params = {'device':device, 'optimizer': optimizer, 'loss_functions': loss_funcs}\n","        # build \n","        model.build(**build_params)\n","\n","        # train\n","        if val_tensor is None:\n","            model.train(train_tensor, batch_size = batch_size,\n","                        num_epochs=num_epochs, verbose=True)\n","        else: \n","            model.train(train_tensor, val_tensor=val_tensor, batch_size=batch_size,\n","                        num_epochs=num_epochs, verbose=True)\n","\n","        # evaluate\n","        _, logits, labels = model.evaluate(test_tensor)\n","        logits, labels = format_outputs(logits, labels, num_tasks)\n","        preds = get_labels_from_multi_logits(logits)\n","        res = print_report_multitask(labels, preds, label_ls )\n","\n","        final_metrics[s] = res\n","\n","        if idx==save_seed:\n","            model.save(save_path)\n","        del model\n","\n","    return final_metrics "],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ef1951dce38e41efa2a8a7a24d229369","8e1c4cb79b1d4499b0232d2957e67fd8","362c9e0467ce4e9291bcac784f627e18","b4fc60761e0d46239eacb75f221ed464","c2a6d35544a149eebcf1590a1158b07e","f6a16236703a4556bcd08f420b57efe8","11211c9126e64ab8a1213c69b5d7e7e0","cad94fd0b4c745c3932d2aaffb26cba6","d399b44d705b4da19a33ed4ea31c14ba","2d11ee017ffe4cb5bace8e78b4321da7","81f1cbb317ac4773851da40c833d125d"]},"id":"_iTm811coQuf","executionInfo":{"status":"ok","timestamp":1631079860466,"user_tz":420,"elapsed":4692751,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"e7814dad-7b43-4bde-cbd3-cea6c61c1c33"},"source":["###### !!CHECK THIS!! ######\n","model_class = RobertaLSTM\n","model_str = 'Roberta_LSTM'\n","###########################\n","target_ls = ['label','target','aggr']\n","num_labels =  [3,4,3]\n","num_tasks = len(num_labels)\n","label_ls = [[0,1,2],[0,1,2,3],[0,1,2]]\n","num_epochs = 5\n","# change correct weights\n","weight_dict = calculate_class_weights(train_dt, target_ls)\n","weight_ls  = [list(weight_dict[k].values()) for k in target_ls]\n","loss_funcs = [CrossEntropyLoss(weight=torch.tensor(w).to(device)) for w in weight_ls]\n","\n","model_params = {'num_labels':num_labels, 'num_tasks':num_tasks, \n","                'max_len':MAX_LEN,'label_ls':label_ls,\n","                'num_epochs':num_epochs, 'loss_funcs':loss_funcs,\n","                }\n","\n","results = run_models(save_path = SAVE_PATH, model_class=model_class, device=device, \n","     train_tensor=train_tensor, test_tensor=test_tensor, \n","    #  seed_list=[101,211],\n","      **model_params )"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["========== Trying seed 101 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef1951dce38e41efa2a8a7a24d229369","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [111.32684406638145, 116.20980527997017, 112.89978414773941]. Validation loss: []. Duration: 181.08280777931213\n","Running Epoch  1\n","Training loss : [89.76621747016907, 77.60200180113316, 83.7922575622797]. Validation loss: []. Duration: 183.09014105796814\n","Running Epoch  2\n","Training loss : [76.98541629314423, 58.4796287715435, 65.06228078901768]. Validation loss: []. Duration: 182.7945911884308\n","Running Epoch  3\n","Training loss : [66.34991861879826, 42.025034757331014, 50.984199933707714]. Validation loss: []. Duration: 182.8249442577362\n","Running Epoch  4\n","Training loss : [60.99205235019326, 32.7362499628216, 41.52474641054869]. Validation loss: []. Duration: 182.7883403301239\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.94588235 0.33333333 0.34328358]\n","recall   : [0.82886598 0.46153846 0.62162162]\n","fscore   : [0.88351648 0.38709677 0.44230769]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.9638009  0.58461538 0.14814815 0.        ]\n","recall   : [0.84860558 0.87356322 0.4        0.        ]\n","fscore   : [0.90254237 0.70046083 0.21621622 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.90952381 0.57615894 0.31034483]\n","recall   : [0.86425339 0.63043478 0.45      ]\n","fscore   : [0.8863109  0.60207612 0.36734694]\n","support  : [442 138  20]\n","========== Trying seed 211 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [114.94759595394135, 120.28970492631197, 114.00618398189545]. Validation loss: []. Duration: 182.63298511505127\n","Running Epoch  1\n","Training loss : [89.11393344402313, 75.78809271752834, 83.0824511051178]. Validation loss: []. Duration: 182.78885197639465\n","Running Epoch  2\n","Training loss : [79.18942806124687, 58.74230134487152, 64.59474094212055]. Validation loss: []. Duration: 182.81631064414978\n","Running Epoch  3\n","Training loss : [69.73266944289207, 46.82100459933281, 50.382176499813795]. Validation loss: []. Duration: 182.88372325897217\n","Running Epoch  4\n","Training loss : [64.96617697924376, 37.708651976659894, 53.52752337232232]. Validation loss: []. Duration: 182.86869883537292\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.94724771 0.36       0.35897436]\n","recall   : [0.85154639 0.57692308 0.37837838]\n","fscore   : [0.89685125 0.44334975 0.36842105]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.94420601 0.65263158 0.16666667 0.        ]\n","recall   : [0.87649402 0.71264368 0.6        0.        ]\n","fscore   : [0.90909091 0.68131868 0.26086957 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.91939547 0.54748603 0.41666667]\n","recall   : [0.82579186 0.71014493 0.5       ]\n","fscore   : [0.87008343 0.61829653 0.45454545]\n","support  : [442 138  20]\n","========== Trying seed 307 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [111.18918406963348, 116.38716571033001, 109.59724271297455]. Validation loss: []. Duration: 182.3735911846161\n","Running Epoch  1\n","Training loss : [86.89794752001762, 74.32635273039341, 77.77511116862297]. Validation loss: []. Duration: 182.42562294006348\n","Running Epoch  2\n","Training loss : [78.11260670423508, 58.298062436282635, 64.79694013297558]. Validation loss: []. Duration: 182.51725220680237\n","Running Epoch  3\n","Training loss : [66.29413821548223, 41.663976188749075, 53.165797129273415]. Validation loss: []. Duration: 182.399640083313\n","Running Epoch  4\n","Training loss : [66.03915815800428, 36.43383138999343, 46.06640212982893]. Validation loss: []. Duration: 182.38044357299805\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.92275574 0.48333333 0.32786885]\n","recall   : [0.91134021 0.37179487 0.54054054]\n","fscore   : [0.91701245 0.42028986 0.40816327]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.9470339  0.65346535 0.15384615 0.        ]\n","recall   : [0.89043825 0.75862069 0.4        0.        ]\n","fscore   : [0.91786448 0.70212766 0.22222222 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.88017429 0.62962963 0.5       ]\n","recall   : [0.91402715 0.61594203 0.15      ]\n","fscore   : [0.89678135 0.62271062 0.23076923]\n","support  : [442 138  20]\n","========== Trying seed 401 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [111.55583372712135, 115.88836634159088, 113.33283650875092]. Validation loss: []. Duration: 182.38463234901428\n","Running Epoch  1\n","Training loss : [86.015306442976, 74.63494184613228, 78.78333479166031]. Validation loss: []. Duration: 182.43910956382751\n","Running Epoch  2\n","Training loss : [75.56213265657425, 57.86696184426546, 64.73885326087475]. Validation loss: []. Duration: 182.33235621452332\n","Running Epoch  3\n","Training loss : [67.7380967438221, 47.12611436471343, 49.47967955470085]. Validation loss: []. Duration: 182.59576272964478\n","Running Epoch  4\n","Training loss : [66.0909375231713, 37.504108127206564, 45.7675277274102]. Validation loss: []. Duration: 182.42570185661316\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.93028322 0.484375   0.2987013 ]\n","recall   : [0.88041237 0.3974359  0.62162162]\n","fscore   : [0.90466102 0.43661972 0.40350877]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.96255507 0.59055118 0.21052632 0.        ]\n","recall   : [0.87051793 0.86206897 0.4        0.        ]\n","fscore   : [0.91422594 0.70093458 0.27586207 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.89244851 0.59722222 0.36842105]\n","recall   : [0.88235294 0.62318841 0.35      ]\n","fscore   : [0.88737201 0.60992908 0.35897436]\n","support  : [442 138  20]\n","========== Trying seed 503 ==========\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [114.37618294358253, 118.02217906713486, 113.2277826666832]. Validation loss: []. Duration: 182.3068106174469\n","Running Epoch  1\n","Training loss : [84.74031034111977, 74.95279301702976, 77.41242265701294]. Validation loss: []. Duration: 182.43622398376465\n","Running Epoch  2\n","Training loss : [74.61048574745655, 54.50885787606239, 60.648317605257034]. Validation loss: []. Duration: 182.59551310539246\n","Running Epoch  3\n","Training loss : [61.971359714865685, 37.12563297152519, 42.44201836362481]. Validation loss: []. Duration: 182.39455556869507\n","Running Epoch  4\n","Training loss : [56.98547484725714, 33.85039523243904, 38.805842120200396]. Validation loss: []. Duration: 182.52776169776917\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.91511387 0.43333333 0.36842105]\n","recall   : [0.91134021 0.33333333 0.56756757]\n","fscore   : [0.91322314 0.37681159 0.44680851]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.96       0.63559322 0.28571429 0.        ]\n","recall   : [0.90836653 0.86206897 0.2        0.        ]\n","fscore   : [0.93346981 0.73170732 0.23529412 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.86266094 0.58267717 0.42857143]\n","recall   : [0.90950226 0.53623188 0.15      ]\n","fscore   : [0.88546256 0.55849057 0.22222222]\n","support  : [442 138  20]\n"]}]},{"cell_type":"code","metadata":{"id":"HPpGBDuFRMcl","executionInfo":{"status":"ok","timestamp":1631079860591,"user_tz":420,"elapsed":133,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":["RESULT_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Results/\"\n","\n","specific_res = get_specific_results(results)\n","specific_res.to_pickle(RESULT_PATH + model_str + '_specific.pkl')\n","\n","macro_res    = get_macro_results(results)\n","macro_res.to_pickle(RESULT_PATH + model_str + '_macro.pkl')"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3V4slhXyCB_","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1631079860704,"user_tz":420,"elapsed":115,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"5fe14e35-9884-4e06-ee7c-80c5002aa34e"},"source":["specific_res"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Normal</th>\n","      <th>Abusive</th>\n","      <th>Hate</th>\n","      <th>Neither/NA</th>\n","      <th>Anti-Asian</th>\n","      <th>Anti-Black</th>\n","      <th>Both</th>\n","      <th>No.Agg.</th>\n","      <th>Some.Agg.</th>\n","      <th>Very.Agg.</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.93 +- 0.01</td>\n","      <td>0.42 +- 0.06</td>\n","      <td>0.34 +- 0.02</td>\n","      <td>0.96 +- 0.01</td>\n","      <td>0.62 +- 0.03</td>\n","      <td>0.19 +- 0.05</td>\n","      <td>--</td>\n","      <td>0.89 +- 0.02</td>\n","      <td>0.59 +- 0.03</td>\n","      <td>0.4 +- 0.06</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.88 +- 0.03</td>\n","      <td>0.43 +- 0.09</td>\n","      <td>0.55 +- 0.09</td>\n","      <td>0.88 +- 0.02</td>\n","      <td>0.81 +- 0.07</td>\n","      <td>0.4 +- 0.13</td>\n","      <td>--</td>\n","      <td>0.88 +- 0.03</td>\n","      <td>0.62 +- 0.06</td>\n","      <td>0.32 +- 0.15</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.9 +- 0.01</td>\n","      <td>0.41 +- 0.03</td>\n","      <td>0.41 +- 0.03</td>\n","      <td>0.92 +- 0.01</td>\n","      <td>0.7 +- 0.02</td>\n","      <td>0.24 +- 0.02</td>\n","      <td>--</td>\n","      <td>0.89 +- 0.01</td>\n","      <td>0.6 +- 0.02</td>\n","      <td>0.33 +- 0.09</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Normal       Abusive  ...     Some.Agg.     Very.Agg.\n","0  0.93 +- 0.01  0.42 +- 0.06  ...  0.59 +- 0.03   0.4 +- 0.06\n","1  0.88 +- 0.03  0.43 +- 0.09  ...  0.62 +- 0.06  0.32 +- 0.15\n","2   0.9 +- 0.01  0.41 +- 0.03  ...   0.6 +- 0.02  0.33 +- 0.09\n","\n","[3 rows x 10 columns]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"BbUQOvoq78Ru","colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"status":"ok","timestamp":1631079860798,"user_tz":420,"elapsed":98,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"7e589892-1e06-46a3-d3d1-d80c6acf1ee4"},"source":["macro_res"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.56 +- 0.01</td>\n","      <td>0.62 +- 0.02</td>\n","      <td>0.59 +- 0.01</td>\n","      <td>0.79 +- 0.01</td>\n","      <td>0.85 +- 0.03</td>\n","      <td>0.82 +- 0.01</td>\n","      <td>0.63 +- 0.02</td>\n","      <td>0.61 +- 0.05</td>\n","      <td>0.62 +- 0.02</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              0             1  ...             7             8\n","0  0.56 +- 0.01  0.62 +- 0.02  ...  0.61 +- 0.05  0.62 +- 0.02\n","\n","[1 rows x 9 columns]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"4pfEs3OfRqRJ","executionInfo":{"status":"ok","timestamp":1631079860799,"user_tz":420,"elapsed":6,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":[""],"execution_count":19,"outputs":[]}]}