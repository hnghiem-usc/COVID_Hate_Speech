{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBerta_LSTM_agg_as_numeric","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pp3_2ZLq6NS","executionInfo":{"status":"ok","timestamp":1631673776660,"user_tz":420,"elapsed":6868,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"95d085ac-d8d9-479a-83d0-f5e4f1d8441a"},"source":["from datetime import datetime\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import re, string\n","import time\n","import random\n","import gc\n","import os, sys\n","import json\n","from copy import deepcopy\n","\n","#####\n","# !pip install gdown\n","!pip install transformers\n","pd.options.display.max_colwidth = 500\n","#####\n","\n","import torch\n","\n","from transformers import BertForSequenceClassification, AdamW , BertConfig, BertModel\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import LineByLineTextDataset\n","\n","from torch.nn import LSTM, Dropout, Linear, BCELoss, CrossEntropyLoss, Softmax, ReLU, LeakyReLU, Tanh, MSELoss\n","from torch.nn.functional import softmax, relu\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.utils.data.dataset import Subset\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n","\n","%load_ext autoreload \n","%autoreload 2"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n","Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzEvkGACWMqX","executionInfo":{"status":"ok","timestamp":1631673776661,"user_tz":420,"elapsed":24,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"07751c35-1024-4efe-8945-dbb894c22876"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUYifLXWu1D0","executionInfo":{"status":"ok","timestamp":1631673776662,"user_tz":420,"elapsed":21,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"f65f4c85-46d7-4587-fbca-583b6a01e3c7"},"source":["!ls \"/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model\""],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["'Bert Models'   models.py   __pycache__   RoBerta  'Roberta Final'\n"]}]},{"cell_type":"code","metadata":{"id":"f6srHxsX4FhM","executionInfo":{"status":"ok","timestamp":1631673776663,"user_tz":420,"elapsed":16,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["PATH_PREFIX = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8keHP8s6Fj4","executionInfo":{"status":"ok","timestamp":1631673777566,"user_tz":420,"elapsed":919,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils')\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model')\n","\n","from models import *\n","from text_processing import *"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFayi-JvrDHx","executionInfo":{"status":"ok","timestamp":1631673778004,"user_tz":420,"elapsed":440,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"6832b650-ba4b-470a-f56e-94136705e2a8"},"source":["# Enable GPU before proceeding\n","device_name = tf.test.gpu_device_name()\n","print(\"Device found:\", device_name)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Device found: /device:GPU:0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLkb-vZhryC-","executionInfo":{"status":"ok","timestamp":1631673778005,"user_tz":420,"elapsed":10,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"6faafbc3-1556-48ed-ae2e-04f90e2616b9"},"source":["# Test if CUDA is available, use it\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","print(\"Using CUDA Device:\", torch.cuda.get_device_name())"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA Device: Tesla K80\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jgmig-gyT5_U"},"source":["# DOWNLOAD AND PREPROCESS DATA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIGMfizwRlrM","executionInfo":{"status":"ok","timestamp":1631673778219,"user_tz":420,"elapsed":13,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"d386b216-2eb6-437b-fb9c-6338aa48161f"},"source":["train_dt, test_dt = load_data(PATH_PREFIX + '/Data/her_agg_final.csv', \n","                   dtype_dict={'tweetid':str, 'label':int, 'target':int, 'aggr':int})\n","\n"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["label\n","0    1894\n","1     304\n","2     202\n","Name: label, dtype: int64\n","target\n","0    2003\n","1     367\n","2      25\n","3       5\n","Name: target, dtype: int64\n","aggr\n","0    1799\n","1     533\n","2      68\n","Name: aggr, dtype: int64\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":196},"id":"EjMqA17amRqF","executionInfo":{"status":"ok","timestamp":1631673778220,"user_tz":420,"elapsed":11,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"00c2514d-3e25-488e-c40f-5cbd7b2409a9"},"source":["train_dt.head(2)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweetid</th>\n","      <th>content</th>\n","      <th>max_speech_type</th>\n","      <th>max_agg_type</th>\n","      <th>max_target_type</th>\n","      <th>label</th>\n","      <th>aggr</th>\n","      <th>target</th>\n","      <th>text</th>\n","      <th>agg_num</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1316836223609118727</td>\n","      <td>How Queen Elizabeth II safely went without a mask as she left lockdown https://t.co/3pAIoBVXJk via @TheNationalNews</td>\n","      <td>normal</td>\n","      <td>not_aggressive</td>\n","      <td>na</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>how queen elizabeth ii safely went without a mask as she left lockdown &lt;url&gt; via &lt;user&gt;</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1318379728642953217</td>\n","      <td>Five ways to grow your business online through the COVID-19 pandemic #Entrepreneur #Startup #Business #Customer #Brand #b2c #yoyakka https://t.co/UEj05D4hVq</td>\n","      <td>normal</td>\n","      <td>not_aggressive</td>\n","      <td>na</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>five ways to grow your business online through the covid19 pandemic entrepreneur startup business customer brand b2c yoy akka &lt;url&gt;</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               tweetid  ... agg_num\n","0  1316836223609118727  ...     0.0\n","1  1318379728642953217  ...     0.0\n","\n","[2 rows x 10 columns]"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"42nIFpyRNTJw"},"source":["## Tokenize state"]},{"cell_type":"code","metadata":{"id":"GPIccBVB_yr1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631673778331,"user_tz":420,"elapsed":120,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"4ac266d4-1129-400a-f2e7-85f51369c6ef"},"source":["# import hashtag_dict \n","HASHTAG_DIR = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils/' \n","with open(HASHTAG_DIR + 'hashtag_dict.json', 'r') as f:\n","    hashtag_dict = json.load(f)\n","\n","# CUDA is memory hungry. Needs to reduce the max_len\n","MAX_LEN = 225\n","print(\"Maximum length of cleaned up dataset is:\", MAX_LEN)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum length of cleaned up dataset is: 225\n"]}]},{"cell_type":"code","metadata":{"id":"CIPViSOuXhTA","executionInfo":{"status":"ok","timestamp":1631673779783,"user_tz":420,"elapsed":1454,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HP_qSMkuohrO","executionInfo":{"status":"ok","timestamp":1631673781890,"user_tz":420,"elapsed":2113,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"2660d81c-7c9b-411c-8ac0-665cc93689eb"},"source":["train_input_ids, train_attn_masks  = tokenizing(train_dt.text.values, tokenizer, MAX_LEN)\n","test_input_ids, test_attn_masks  = tokenizing(test_dt.text.values, tokenizer, MAX_LEN)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Total time taken:  1.6772887706756592\n","Total time taken:  0.38543224334716797\n"]}]},{"cell_type":"code","metadata":{"id":"5UaN8FNEUbn9","executionInfo":{"status":"ok","timestamp":1631673781891,"user_tz":420,"elapsed":8,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["varlist = ['agg_num']\n","_, train_tensor = arrange_tensor(train_dt, train_input_ids, train_attn_masks ,varlist=varlist)\n","_, test_tensor  = arrange_tensor(test_dt, test_input_ids, test_attn_masks ,varlist=varlist)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jKEuQnyKzKaP"},"source":["# BUILD MODEL"]},{"cell_type":"markdown","metadata":{"id":"ptpLoOHEKWBx"},"source":["In this model, we will build an LSTM model leveraging BERT base hidden data on top. The key difference compared to simpleBert is that we need to save the parameters."]},{"cell_type":"code","metadata":{"id":"t0NtCmP8NM8B","executionInfo":{"status":"ok","timestamp":1631673781891,"user_tz":420,"elapsed":6,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Saved Parameters/\""],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"hR85mdRorT83","executionInfo":{"status":"ok","timestamp":1631673782040,"user_tz":420,"elapsed":155,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["class RobertaLSTM_Reg(Roberta_base):\n","    def __init__(self, bert_type, num_labels:list, max_len, attn_yn=False, dropout_rate = 0.2):\n","        super(RobertaLSTM_Reg, self).__init__(bert_type, num_labels, max_len, attn_yn, dropout_rate)\n","        self.model_type = 'Roberta_LSTM_reg'\n","        self.bert_type = bert_type\n","        if len(num_labels) == 1:\n","            print('Single-task training')\n","        else: \n","            print('Multitask training: {} tasks '.format(len(num_labels)))\n","\n","        self.num_labels = num_labels\n","        self.num_tasks = len(self.num_labels)\n","        self.train_losses = [[]] * self.num_tasks\n","        self.val_losses = [[]] * self.num_tasks\n","\n","        self.attn_yn = attn_yn\n","        self.max_len = torch.tensor([max_len])\n","        self.dropout_rate = 0.2\n","        # BERT \n","        self.bert = RobertaModel.from_pretrained(self.bert_type,\n","                                                #    output_attentions = attn_yn, \n","                                                #    output_hidden_states=True\n","                                              )\n","        self.dropout = Dropout(p = self.dropout_rate)\n","        self.device_objects = {'self.bert': self.bert, 'self.dropout': self.dropout}\n","        ## LSTM\n","        self.lstm_input_size = self.bert.config.hidden_size\n","        self.lstm_hidden_size = self.bert.config.hidden_size\n","        self.lstm = LSTM(input_size = self.lstm_input_size,\n","                        hidden_size = self.lstm_hidden_size, \n","                         bidirectional = True )\n","        self.device_objects['self.lstm'] = self.lstm\n","        # Classifier layers\n","        self.fc_dict = {\n","                        'self.fc'+str(i) : Linear(in_features=2*self.lstm_hidden_size,\n","                                            out_features = n,\n","                                            bias=True)\n","                        for i,n in enumerate(self.num_labels)\n","                     }\n","        self.device_objects.update(self.fc_dict)\n","        # initialize all weights and bias\n","        self.initialize_parameters()\n","\n","\n","    def forward(self, batch):\n","        \"\"\"\n","        One pass forward using data in batch. \n","        NOTE: Should ONLY BE USED inside evaluate or train function\n","        @param batch: tuple or list, tuples of input_ids, attention_maksks, labels, and sentence legnth \n","        \"\"\"\n","        sent_lens = batch[-1]\n","        batch = tuple(t.to(self.device) for t in batch[:-1])\n","        input_ids, attn_masks = batch[:2]\n","        labels = batch[2:]\n","        # print(\"sent_lens\", sent_lens)\n","\n","        # Bert\n","        bert_outputs = self.bert(input_ids, token_type_ids=None, \n","                            attention_mask = attn_masks)\n","        \n","        # LSTM \n","        encoded_layers = bert_outputs.last_hidden_state\n","        encoded_layers = encoded_layers.permute(1,0,2)\n","        enc_hidden, (last_hidden, last_cell) = self.lstm(pack_padded_sequence(encoded_layers,\n","                                                                                      sent_lens,\n","                                                                                      enforce_sorted=False))\n","        # Flatten \n","        output_hidden = torch.cat((last_hidden[0,:,:], last_hidden[1,:,:]), dim=1)\n","        # Drop out\n","        output_hidden = self.dropout(output_hidden)\n","        # Classifier layer\n","        outputs = []\n","        for i in range(self.num_tasks):\n","            output_temp = self.fc_dict['self.fc'+str(i)](output_hidden)\n","            # If using Cross-Entropy loss, then do NOT normalize output\n","            outputs.append(output_temp)\n","\n","        # return output_hidden\n","        return outputs, labels\n","\n","\n","    def train(self, X, num_epochs, batch_size=20, scheduler=None, X_val = None, verbose=False): \n","        \"\"\"\n","        param X: tensor, dataset that contains all inputs, attention masks, and labels \n","        \"\"\"\n","        self.scheduler = scheduler\n","        # create Data loader \n","        train_dataloader = DataLoader(X,\n","                         # sampler=RandomSampler(X),\n","                          shuffle=False,\n","                          batch_size=batch_size)\n","        \n","        loss_history = {'train': [], 'val': []}\n","\n","        for e in range(num_epochs):\n","            print(\"Running Epoch \", e)\n","            # Switch to train mode\n","            [x.train() for x in self.device_objects.values()] \n","\n","            train_loss = [0] * self.num_tasks\n","            start =  time.time()\n","\n","            for step, batch in enumerate(train_dataloader):\n","                self.bert.zero_grad()\n","\n","                logits, labels = self.forward(batch)\n","                overall_loss = 0\n","                # Calculate losses\n","                for i in range(self.num_tasks):\n","                    batch_loss = self.loss_functions[i](logits[i].float(), labels[i].float())\n","                    overall_loss += batch_loss\n","                    train_loss[i] += batch_loss.cpu().item()\n","\n","                # backgrad to update parameters\n","                overall_loss.backward()\n","                [torch.nn.utils.clip_grad_norm_(layer.parameters(), 1.0) for layer in self.device_objects.values()] \n","                self.opt.step()\n","                \n","                # weight decay \n","                if self.scheduler is not None:\n","                    self.scheduler.step()\n","\n","            # Results on valdiation set\n","            val_loss = []\n","            if X_val is not None:\n","                val_loss, _, _ = self.evaluate(X_val, batch_size)\n","\n","            # Append to overal_losses\n","            for i in range(self.num_tasks):\n","                self.train_losses[i].append(train_loss[i])\n","                if X_val is not None: \n","                    self.val_losses[i].append(val_loss[i])\n","                \n","            duration = time.time()- start\n","            if verbose: \n","                print(\"Training loss : {}. Validation loss: {}. Duration: {}\".format(train_loss,\n","                                                                                 val_loss,  \n","                                                                                 duration))\n","            loss_history['train'].append(train_loss)\n","            loss_history['val'].append(val_loss)\n","\n","        return loss_history\n","\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNkcn9ptRjsh","executionInfo":{"status":"ok","timestamp":1631673782195,"user_tz":420,"elapsed":157,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["from sklearn.metrics import mean_absolute_error as MAE\n","def get_labels_from_reg_logits(logits): \n","    labels = []\n","    for i in logits:\n","         for j in i:\n","            labels.append(j[0])\n","    return labels\n","    "],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"2K9sRD_qgCEY","executionInfo":{"status":"ok","timestamp":1631673782345,"user_tz":420,"elapsed":153,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["def run_models(save_path:str, model_class, num_labels, num_tasks, max_len,\n","        num_epochs, loss_funcs:list, device,\n","        train_tensor, test_tensor, val_tensor=None, batch_size=20, label_ls = [[0,1,2]],\n","        seed_list = [101,211,307,401,503], save_seed=0):\n","\n","    final_metrics = dict()\n","    # Iterate through seeds\n","    for idx, s in enumerate(seed_list):\n","        print('========== Trying seed', s, '==========')\n","        # Reset the seed\n","        seed = s\n","        random.seed(seed)\n","        os.environ['PYTHONHASHSEED'] = str(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = True\n","    \n","        # print(\"Random num: \", random.randint(1,1000))\n","        \n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        # Initialize model\n","        model = model_class('roberta-base', num_labels=num_labels, max_len=max_len)\n","        optimizer = AdamW(model.parameters(),lr=2e-5, eps=1e-6, weight_decay=1e-3)\n","        build_params = {'device':device, 'optimizer': optimizer, 'loss_functions': loss_funcs}\n","        # build \n","        model.build(**build_params)\n","\n","        # train\n","        if val_tensor is None:\n","            model.train(train_tensor, batch_size = batch_size,\n","                        num_epochs=num_epochs, verbose=True)\n","        else: \n","            model.train(train_tensor, val_tensor=val_tensor, batch_size=batch_size,\n","                        num_epochs=num_epochs, verbose=True)\n","\n","        # evaluate\n","        _, logits, labels = model.evaluate(test_tensor)\n","        logits, labels = format_outputs(logits, labels, num_tasks)\n","        preds = get_labels_from_reg_logits(logits)\n","        # res = print_report_multitask(labels, preds, label_ls )\n","\n","        # final_metrics[s] = res\n","\n","        # if idx==save_seed:\n","        #     model.save(save_path)\n","        # del model\n","\n","    # return final_metrics \n","    return preds, labels"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_iTm811coQuf","executionInfo":{"status":"ok","timestamp":1631674716171,"user_tz":420,"elapsed":933828,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"784ae16e-762f-4615-d984-ea2c6a37e2ef"},"source":["###### !!CHECK THIS!! ######\n","model_class = RobertaLSTM_Reg\n","model_str = 'Roberta_LSTM_Reg'\n","###########################\n","target_ls = ['agg_num']\n","num_labels =  [1]\n","num_tasks = len(num_labels)\n","label_ls = [[0.,1.,2.]]\n","num_epochs = 5\n","# change correct weights\n","weight_dict = calculate_class_weights(train_dt, target_ls)\n","weight_ls  = [list(weight_dict[k].values()) for k in target_ls]\n","# loss_funcs = [MSELoss(weight=torch.tensor(w).to(device)) for w in weight_ls]\n","loss_funcs = [MSELoss()]\n","\n","model_params = {'num_labels':num_labels, 'num_tasks':num_tasks, \n","                'max_len':MAX_LEN,'label_ls':label_ls,\n","                'num_epochs':num_epochs, 'loss_funcs':loss_funcs,\n","                }\n","preds, labels = run_models(save_path = SAVE_PATH, model_class=model_class, device=device, \n","     train_tensor=train_tensor, test_tensor=test_tensor, \n","     seed_list=[101],\n","      **model_params )"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["========== Trying seed 101 ==========\n","Sigle-task training\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Single-task training\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Training loss : [57.37283453345299]. Validation loss: []. Duration: 182.2247931957245\n","Running Epoch  1\n","Training loss : [56.24239167571068]. Validation loss: []. Duration: 182.0948121547699\n","Running Epoch  2\n","Training loss : [56.2748387157917]. Validation loss: []. Duration: 182.36680054664612\n","Running Epoch  3\n","Training loss : [55.871003180742264]. Validation loss: []. Duration: 182.1095950603485\n","Running Epoch  4\n","Training loss : [55.7120094448328]. Validation loss: []. Duration: 182.083801984787\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__7fQJ05S_VM","executionInfo":{"status":"ok","timestamp":1631674726288,"user_tz":420,"elapsed":217,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"0e74dc61-8775-47f2-a730-e1909564ae04"},"source":["MAE(labels[0], preds)"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5733330641521348"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"HPpGBDuFRMcl","executionInfo":{"status":"ok","timestamp":1631674716173,"user_tz":420,"elapsed":11,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":["RESULT_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Results/\"\n","\n","# specific_res = get_specific_results(results)\n","# specific_res.to_pickle(RESULT_PATH + model_str + '_specific.pkl')\n","\n","# macro_res    = get_macro_results(results)\n","# macro_res.to_pickle(RESULT_PATH + model_str + '_macro.pkl')"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"4pfEs3OfRqRJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631674765342,"user_tz":420,"elapsed":456,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}},"outputId":"6e3056cc-d3b3-4ec9-f574-f2ed4b70fe07"},"source":["preds"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.5288869738578796,\n"," 0.523455023765564,\n"," 0.5048137307167053,\n"," 0.47836872935295105,\n"," 0.5484133362770081,\n"," 0.4577631950378418,\n"," 0.5080379247665405,\n"," 0.5372573733329773,\n"," 0.4912300407886505,\n"," 0.5144142508506775,\n"," 0.5288445949554443,\n"," 0.5391861200332642,\n"," 0.47656023502349854,\n"," 0.47783738374710083,\n"," 0.4621857702732086,\n"," 0.4871085286140442,\n"," 0.51888108253479,\n"," 0.5149614214897156,\n"," 0.4905957877635956,\n"," 0.541279673576355,\n"," 0.4675861895084381,\n"," 0.5062057971954346,\n"," 0.47428515553474426,\n"," 0.5206024050712585,\n"," 0.5232827663421631,\n"," 0.4920217990875244,\n"," 0.5442163944244385,\n"," 0.5352535843849182,\n"," 0.515845537185669,\n"," 0.5310103297233582,\n"," 0.5237704515457153,\n"," 0.5282254815101624,\n"," 0.5198144316673279,\n"," 0.4792257249355316,\n"," 0.4933842122554779,\n"," 0.6156231164932251,\n"," 0.6397431492805481,\n"," 0.5177190899848938,\n"," 0.5173441171646118,\n"," 0.5456368327140808,\n"," 0.54314124584198,\n"," 0.4353029429912567,\n"," 0.5268754363059998,\n"," 0.5244839191436768,\n"," 0.5440486669540405,\n"," 0.556231677532196,\n"," 0.5126330256462097,\n"," 0.5851161479949951,\n"," 0.4954754710197449,\n"," 0.5312751531600952,\n"," 0.512759804725647,\n"," 0.5358939170837402,\n"," 0.5093408226966858,\n"," 0.5057292580604553,\n"," 0.4937860071659088,\n"," 0.5353716611862183,\n"," 0.4548938274383545,\n"," 0.49572810530662537,\n"," 0.5563148260116577,\n"," 0.5036735534667969,\n"," 0.5100500583648682,\n"," 0.5485808849334717,\n"," 0.5138518810272217,\n"," 0.5810357928276062,\n"," 0.5131571292877197,\n"," 0.5229909420013428,\n"," 0.608755350112915,\n"," 0.5361676812171936,\n"," 0.5293176770210266,\n"," 0.48622921109199524,\n"," 0.5129433274269104,\n"," 0.5107218027114868,\n"," 0.563412606716156,\n"," 0.5412654280662537,\n"," 0.5159674882888794,\n"," 0.5244228839874268,\n"," 0.5651243329048157,\n"," 0.5255328416824341,\n"," 0.5181949138641357,\n"," 0.5001216530799866,\n"," 0.5135484933853149,\n"," 0.5387718677520752,\n"," 0.45938047766685486,\n"," 0.5367299914360046,\n"," 0.5411112904548645,\n"," 0.4843672811985016,\n"," 0.4803263545036316,\n"," 0.5610908269882202,\n"," 0.4824543297290802,\n"," 0.5525412559509277,\n"," 0.535045325756073,\n"," 0.5245258212089539,\n"," 0.5105912685394287,\n"," 0.5106252431869507,\n"," 0.5441409349441528,\n"," 0.5318848490715027,\n"," 0.5030555129051208,\n"," 0.5237099528312683,\n"," 0.5363724231719971,\n"," 0.511049211025238,\n"," 0.4946710169315338,\n"," 0.48916468024253845,\n"," 0.5130912065505981,\n"," 0.48846235871315,\n"," 0.5011282563209534,\n"," 0.5199331045150757,\n"," 0.5139779448509216,\n"," 0.4959152638912201,\n"," 0.5015936493873596,\n"," 0.5187080502510071,\n"," 0.5904747247695923,\n"," 0.4862665832042694,\n"," 0.5481706857681274,\n"," 0.4616185128688812,\n"," 0.540136992931366,\n"," 0.5249152779579163,\n"," 0.5397500991821289,\n"," 0.4960850477218628,\n"," 0.5209047198295593,\n"," 0.5026485919952393,\n"," 0.5275914669036865,\n"," 0.49976596236228943,\n"," 0.5048201084136963,\n"," 0.5268890261650085,\n"," 0.5849676132202148,\n"," 0.4826186001300812,\n"," 0.47879892587661743,\n"," 0.5196459293365479,\n"," 0.4773642122745514,\n"," 0.5463692545890808,\n"," 0.535323441028595,\n"," 0.489373117685318,\n"," 0.5515409708023071,\n"," 0.48902222514152527,\n"," 0.5579060316085815,\n"," 0.47407108545303345,\n"," 0.5503978729248047,\n"," 0.5020937323570251,\n"," 0.5389866828918457,\n"," 0.525421142578125,\n"," 0.5670416951179504,\n"," 0.476534366607666,\n"," 0.5278876423835754,\n"," 0.5177797079086304,\n"," 0.5373884439468384,\n"," 0.46068722009658813,\n"," 0.4856346845626831,\n"," 0.46987730264663696,\n"," 0.5640980005264282,\n"," 0.4997982978820801,\n"," 0.5010526180267334,\n"," 0.5364320278167725,\n"," 0.49600809812545776,\n"," 0.5758299827575684,\n"," 0.4868190884590149,\n"," 0.5224329829216003,\n"," 0.5018870830535889,\n"," 0.5445822477340698,\n"," 0.5951948165893555,\n"," 0.4883091449737549,\n"," 0.5326448678970337,\n"," 0.5559208393096924,\n"," 0.4963516592979431,\n"," 0.45511192083358765,\n"," 0.5187879204750061,\n"," 0.5166858434677124,\n"," 0.5197075009346008,\n"," 0.48849496245384216,\n"," 0.5467207431793213,\n"," 0.5088505148887634,\n"," 0.5427356958389282,\n"," 0.5028896331787109,\n"," 0.6092501878738403,\n"," 0.48949795961380005,\n"," 0.4817286431789398,\n"," 0.5088361501693726,\n"," 0.48687630891799927,\n"," 0.5499734282493591,\n"," 0.4826691150665283,\n"," 0.4741264283657074,\n"," 0.5169743299484253,\n"," 0.4723999798297882,\n"," 0.5079692602157593,\n"," 0.5294026136398315,\n"," 0.5257082581520081,\n"," 0.5160954594612122,\n"," 0.5358982086181641,\n"," 0.48758745193481445,\n"," 0.5393546223640442,\n"," 0.5286708474159241,\n"," 0.5814547538757324,\n"," 0.4986706078052521,\n"," 0.5368660688400269,\n"," 0.46707701683044434,\n"," 0.448567271232605,\n"," 0.5429469347000122,\n"," 0.46931037306785583,\n"," 0.5154688358306885,\n"," 0.5579106211662292,\n"," 0.5351119041442871,\n"," 0.518259584903717,\n"," 0.4961445927619934,\n"," 0.5183261632919312,\n"," 0.484954833984375,\n"," 0.5299121141433716,\n"," 0.5367715358734131,\n"," 0.5465807318687439,\n"," 0.5109837651252747,\n"," 0.46856769919395447,\n"," 0.49292463064193726,\n"," 0.5210707783699036,\n"," 0.5591994524002075,\n"," 0.563806414604187,\n"," 0.5215407609939575,\n"," 0.5627013444900513,\n"," 0.559589684009552,\n"," 0.5288635492324829,\n"," 0.5232307314872742,\n"," 0.5182368159294128,\n"," 0.5477654337882996,\n"," 0.5271657109260559,\n"," 0.49073147773742676,\n"," 0.525995671749115,\n"," 0.47528591752052307,\n"," 0.5189250111579895,\n"," 0.4978867769241333,\n"," 0.49989861249923706,\n"," 0.5576969385147095,\n"," 0.5690478086471558,\n"," 0.564984142780304,\n"," 0.4879954159259796,\n"," 0.4599275290966034,\n"," 0.5303664207458496,\n"," 0.5289012789726257,\n"," 0.4971272945404053,\n"," 0.509584903717041,\n"," 0.49699658155441284,\n"," 0.5169385075569153,\n"," 0.4999663233757019,\n"," 0.4946689009666443,\n"," 0.47567620873451233,\n"," 0.5152724385261536,\n"," 0.5284870862960815,\n"," 0.4958750605583191,\n"," 0.44251513481140137,\n"," 0.47317108511924744,\n"," 0.5593533515930176,\n"," 0.5253977179527283,\n"," 0.5599437952041626,\n"," 0.5381976962089539,\n"," 0.45071160793304443,\n"," 0.5396914482116699,\n"," 0.49246859550476074,\n"," 0.49545058608055115,\n"," 0.47513407468795776,\n"," 0.5207133889198303,\n"," 0.4965585470199585,\n"," 0.5423057079315186,\n"," 0.5138922929763794,\n"," 0.536739706993103,\n"," 0.48832130432128906,\n"," 0.5054338574409485,\n"," 0.5486176013946533,\n"," 0.540917694568634,\n"," 0.5063462257385254,\n"," 0.48528727889060974,\n"," 0.5389376282691956,\n"," 0.4947735369205475,\n"," 0.5237295627593994,\n"," 0.5281936526298523,\n"," 0.4935012459754944,\n"," 0.5256208777427673,\n"," 0.5015294551849365,\n"," 0.48706454038619995,\n"," 0.5179585814476013,\n"," 0.5359486937522888,\n"," 0.5128699541091919,\n"," 0.540519654750824,\n"," 0.5578222274780273,\n"," 0.5498154759407043,\n"," 0.5033406019210815,\n"," 0.47803452610969543,\n"," 0.5310713052749634,\n"," 0.5550094842910767,\n"," 0.5873075127601624,\n"," 0.5216934084892273,\n"," 0.5033779144287109,\n"," 0.5070420503616333,\n"," 0.4825138449668884,\n"," 0.5204854607582092,\n"," 0.5039793848991394,\n"," 0.5618500709533691,\n"," 0.5285413265228271,\n"," 0.5205596685409546,\n"," 0.505247950553894,\n"," 0.5239287614822388,\n"," 0.4948920011520386,\n"," 0.5220946669578552,\n"," 0.49904724955558777,\n"," 0.5186765193939209,\n"," 0.4412115514278412,\n"," 0.5451535582542419,\n"," 0.5948047041893005,\n"," 0.5028632879257202,\n"," 0.5470237731933594,\n"," 0.5246546268463135,\n"," 0.5283905863761902,\n"," 0.5321569442749023,\n"," 0.4890153110027313,\n"," 0.5751378536224365,\n"," 0.5076533555984497,\n"," 0.42595118284225464,\n"," 0.5523009896278381,\n"," 0.5967496037483215,\n"," 0.5550640821456909,\n"," 0.5014084577560425,\n"," 0.5612873435020447,\n"," 0.485041081905365,\n"," 0.49802833795547485,\n"," 0.5432471632957458,\n"," 0.5083231329917908,\n"," 0.5027062892913818,\n"," 0.47434520721435547,\n"," 0.5146135091781616,\n"," 0.522689938545227,\n"," 0.5089146494865417,\n"," 0.5535012483596802,\n"," 0.4903920590877533,\n"," 0.5098419189453125,\n"," 0.4698258340358734,\n"," 0.5583719611167908,\n"," 0.4982633590698242,\n"," 0.47770956158638,\n"," 0.4731501340866089,\n"," 0.5303422212600708,\n"," 0.5046690106391907,\n"," 0.49035364389419556,\n"," 0.5205585956573486,\n"," 0.5318710803985596,\n"," 0.5106091499328613,\n"," 0.48743677139282227,\n"," 0.4864950180053711,\n"," 0.5006532669067383,\n"," 0.5524051189422607,\n"," 0.475576788187027,\n"," 0.48630574345588684,\n"," 0.41458022594451904,\n"," 0.5155574679374695,\n"," 0.4792032837867737,\n"," 0.5654266476631165,\n"," 0.5305623412132263,\n"," 0.46611258387565613,\n"," 0.5374993681907654,\n"," 0.6059812903404236,\n"," 0.4977473020553589,\n"," 0.536136269569397,\n"," 0.5435715913772583,\n"," 0.538554310798645,\n"," 0.5187259912490845,\n"," 0.46775293350219727,\n"," 0.5015235543251038,\n"," 0.5015946626663208,\n"," 0.5117769837379456,\n"," 0.5288663506507874,\n"," 0.49547505378723145,\n"," 0.48485666513442993,\n"," 0.5430611371994019,\n"," 0.5250025987625122,\n"," 0.47080981731414795,\n"," 0.5459666848182678,\n"," 0.47690990567207336,\n"," 0.5009106397628784,\n"," 0.5246433019638062,\n"," 0.49898940324783325,\n"," 0.5426787734031677,\n"," 0.5020225048065186,\n"," 0.561866044998169,\n"," 0.5445063710212708,\n"," 0.5369414687156677,\n"," 0.547721803188324,\n"," 0.538128674030304,\n"," 0.5135319828987122,\n"," 0.4612569510936737,\n"," 0.5371485948562622,\n"," 0.5072914361953735,\n"," 0.5204747915267944,\n"," 0.4760356545448303,\n"," 0.522861123085022,\n"," 0.5072311162948608,\n"," 0.5331503748893738,\n"," 0.4809955358505249,\n"," 0.5307051539421082,\n"," 0.5285831689834595,\n"," 0.5010839700698853,\n"," 0.5109033584594727,\n"," 0.5448822975158691,\n"," 0.5290579199790955,\n"," 0.5478941202163696,\n"," 0.4999721348285675,\n"," 0.5020482540130615,\n"," 0.4504729211330414,\n"," 0.543921172618866,\n"," 0.5333795547485352,\n"," 0.4800683259963989,\n"," 0.5053489804267883,\n"," 0.4704388380050659,\n"," 0.5173547863960266,\n"," 0.523651123046875,\n"," 0.5136929154396057,\n"," 0.4795968532562256,\n"," 0.5119893550872803,\n"," 0.5078760981559753,\n"," 0.4844934642314911,\n"," 0.5036327838897705,\n"," 0.5157761573791504,\n"," 0.5213721394538879,\n"," 0.5455768704414368,\n"," 0.47124817967414856,\n"," 0.5419139862060547,\n"," 0.5047752857208252,\n"," 0.49714404344558716,\n"," 0.5354531407356262,\n"," 0.42197564244270325,\n"," 0.5251175761222839,\n"," 0.5273332595825195,\n"," 0.5000935792922974,\n"," 0.5419012308120728,\n"," 0.4741869866847992,\n"," 0.4987834095954895,\n"," 0.5302082896232605,\n"," 0.5020765066146851,\n"," 0.5555717349052429,\n"," 0.5360845923423767,\n"," 0.508700430393219,\n"," 0.5250992774963379,\n"," 0.5497283935546875,\n"," 0.5324626564979553,\n"," 0.5199463963508606,\n"," 0.5280932188034058,\n"," 0.5286023616790771,\n"," 0.5025210976600647,\n"," 0.4960293173789978,\n"," 0.5484293699264526,\n"," 0.5507938861846924,\n"," 0.48720431327819824,\n"," 0.5435950756072998,\n"," 0.532274067401886,\n"," 0.49859848618507385,\n"," 0.5336164832115173,\n"," 0.4946872889995575,\n"," 0.5137144327163696,\n"," 0.48549509048461914,\n"," 0.5310840010643005,\n"," 0.5231620073318481,\n"," 0.5696010589599609,\n"," 0.5040069222450256,\n"," 0.514864444732666,\n"," 0.5214770436286926,\n"," 0.4880249500274658,\n"," 0.5372409224510193,\n"," 0.5709797739982605,\n"," 0.5200742483139038,\n"," 0.518613338470459,\n"," 0.5360985398292542,\n"," 0.47233501076698303,\n"," 0.5013883709907532,\n"," 0.4970704913139343,\n"," 0.47374528646469116,\n"," 0.4768751561641693,\n"," 0.4949004650115967,\n"," 0.4711875915527344,\n"," 0.4860594570636749,\n"," 0.5039107799530029,\n"," 0.49744996428489685,\n"," 0.5322428941726685,\n"," 0.5122905969619751,\n"," 0.5241751074790955,\n"," 0.481201171875,\n"," 0.5192846059799194,\n"," 0.5014263987541199,\n"," 0.5171703696250916,\n"," 0.5573709607124329,\n"," 0.48704400658607483,\n"," 0.5028209686279297,\n"," 0.48015087842941284,\n"," 0.4917154014110565,\n"," 0.5162184834480286,\n"," 0.4966476261615753,\n"," 0.4982454180717468,\n"," 0.5204134583473206,\n"," 0.5075291991233826,\n"," 0.4762553870677948,\n"," 0.5204446315765381,\n"," 0.526674747467041,\n"," 0.5156751275062561,\n"," 0.5820586681365967,\n"," 0.5045653581619263,\n"," 0.47050872445106506,\n"," 0.4961822032928467,\n"," 0.46774062514305115,\n"," 0.5024251937866211,\n"," 0.5189943909645081,\n"," 0.5283021330833435,\n"," 0.5010461211204529,\n"," 0.5440495610237122,\n"," 0.5084119439125061,\n"," 0.5241696238517761,\n"," 0.46383222937583923,\n"," 0.49136149883270264,\n"," 0.5467830896377563,\n"," 0.5111597776412964,\n"," 0.48796546459198,\n"," 0.4695141017436981,\n"," 0.5230681896209717,\n"," 0.49406376481056213,\n"," 0.47366830706596375,\n"," 0.4835907816886902,\n"," 0.4878225028514862,\n"," 0.49314549565315247,\n"," 0.48777541518211365,\n"," 0.5281013250350952,\n"," 0.4765952229499817,\n"," 0.4669393002986908,\n"," 0.5071678161621094,\n"," 0.4762895405292511,\n"," 0.5007693767547607,\n"," 0.4978877305984497,\n"," 0.5061133503913879,\n"," 0.48774853348731995,\n"," 0.4706614017486572,\n"," 0.5053508281707764,\n"," 0.5547559261322021,\n"," 0.5142139196395874,\n"," 0.520383894443512,\n"," 0.47401633858680725,\n"," 0.5398269891738892,\n"," 0.5282792448997498,\n"," 0.4971022605895996,\n"," 0.5711269378662109,\n"," 0.4886243939399719,\n"," 0.5276187062263489,\n"," 0.5105607509613037,\n"," 0.5441720485687256,\n"," 0.5361142158508301,\n"," 0.5461582541465759,\n"," 0.5295437574386597,\n"," 0.5158261060714722,\n"," 0.49946755170822144,\n"," 0.492343544960022,\n"," 0.49906983971595764,\n"," 0.46457618474960327,\n"," 0.49429571628570557,\n"," 0.5147228240966797,\n"," 0.5201687216758728,\n"," 0.5113399624824524,\n"," 0.6011613607406616,\n"," 0.5057830214500427,\n"," 0.5038468837738037,\n"," 0.5085307359695435,\n"," 0.5106604695320129,\n"," 0.4894317388534546,\n"," 0.5022950172424316,\n"," 0.5366309881210327,\n"," 0.5095276236534119,\n"," 0.526187002658844,\n"," 0.4954049587249756,\n"," 0.4868224561214447,\n"," 0.5129538774490356,\n"," 0.5222269296646118,\n"," 0.5025964975357056,\n"," 0.5707017183303833,\n"," 0.5258927941322327,\n"," 0.5058245062828064,\n"," 0.5459235310554504,\n"," 0.5111956596374512,\n"," 0.5473698973655701,\n"," 0.5385591983795166,\n"," 0.48945173621177673,\n"," 0.5192294120788574,\n"," 0.5219240784645081,\n"," 0.5239365696907043,\n"," 0.4851773977279663,\n"," 0.5197492837905884,\n"," 0.46556127071380615,\n"," 0.5013884902000427,\n"," 0.552436888217926,\n"," 0.49786561727523804,\n"," 0.5354796051979065,\n"," 0.5224348902702332,\n"," 0.5289539694786072,\n"," 0.49443164467811584,\n"," 0.4892018735408783,\n"," 0.5012707710266113,\n"," 0.5268451571464539,\n"," 0.5153074860572815,\n"," 0.49070072174072266,\n"," 0.5091293454170227,\n"," 0.5201007723808289,\n"," 0.4988500773906708,\n"," 0.5407275557518005]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"9BlhTOGM3qna","executionInfo":{"status":"ok","timestamp":1631674716174,"user_tz":420,"elapsed":11,"user":{"displayName":"Huy Nghiem","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06903637911945605583"}}},"source":[""],"execution_count":20,"outputs":[]}]}