{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBerta_LSTM_scheduler_final","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ae1b868d52ee40bd99b6aab8c96b0ef9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4f2641babf634cf7b7980ab8acad6c69","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cca313ab39e5477cae4c7dffd33a198a","IPY_MODEL_6e26f04c4f39472d9af38f8e9f0a8e26","IPY_MODEL_2131435212dc4a6181a69326cdb61b8e"]}},"4f2641babf634cf7b7980ab8acad6c69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cca313ab39e5477cae4c7dffd33a198a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_812d982eefae418ea3ef0c089e05ebf9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f9ddc109af934b6aafba23a4370f7bcf"}},"6e26f04c4f39472d9af38f8e9f0a8e26":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6c96f89a1b0a453b98159171d7670ed4","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":898823,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":898823,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_38f1acfacdc74089ac4ad100364ba6c0"}},"2131435212dc4a6181a69326cdb61b8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_703838841767410ba541c19e1b9295a9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 899k/899k [00:00&lt;00:00, 1.43MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8e6c2cba0c9e4a54aeda0bbe40ec98f3"}},"812d982eefae418ea3ef0c089e05ebf9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f9ddc109af934b6aafba23a4370f7bcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c96f89a1b0a453b98159171d7670ed4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"38f1acfacdc74089ac4ad100364ba6c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"703838841767410ba541c19e1b9295a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8e6c2cba0c9e4a54aeda0bbe40ec98f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a0801124e4804edc8e7df8279bd31dd1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_937e1e12c4e646679e42ad1f46f3c941","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5c8159c8fcfa4a8d8fde225386243d89","IPY_MODEL_9c126273f226451ab0713c5b76bddbc9","IPY_MODEL_a6d99667508e4b2889eb44464e693fb6"]}},"937e1e12c4e646679e42ad1f46f3c941":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5c8159c8fcfa4a8d8fde225386243d89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_530869a160884a09941a87cd4349b809","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ff61733887854fe893bdb53165d5fdf8"}},"9c126273f226451ab0713c5b76bddbc9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_36751a2791e64c9d9aa65c08c732656c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1e7ef4395521496f907711635a3a7f79"}},"a6d99667508e4b2889eb44464e693fb6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b7c74e6088db4a6fa5fe740b4f55139b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:00&lt;00:00, 897kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b69637095d88496ebd5490e2f89ce981"}},"530869a160884a09941a87cd4349b809":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ff61733887854fe893bdb53165d5fdf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"36751a2791e64c9d9aa65c08c732656c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1e7ef4395521496f907711635a3a7f79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b7c74e6088db4a6fa5fe740b4f55139b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b69637095d88496ebd5490e2f89ce981":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2190ae2583d146138f46ca84bc43923d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_51d0a9dfd0d747c48854f17c3b4627ec","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_26ffd5c2a4244a10a77c6dfb6325364a","IPY_MODEL_e9d1c6a93b7648d3a3ec95674d823869","IPY_MODEL_d0ee4f72c1ba42238b10f7866781a98a"]}},"51d0a9dfd0d747c48854f17c3b4627ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"26ffd5c2a4244a10a77c6dfb6325364a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d9023b2d461a429086c214c367743bb4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_70284b7ca1924159b2904b45e5269437"}},"e9d1c6a93b7648d3a3ec95674d823869":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6d70a7e87f604e23bfd9ffd1e8071bad","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1355863,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1355863,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e87ceec4df084261a92a51944956520d"}},"d0ee4f72c1ba42238b10f7866781a98a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_65ba0538ab62417cb8a73ca950514fcf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.36M/1.36M [00:00&lt;00:00, 2.81MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d80a7d252e7c48fc991c327df68a789a"}},"d9023b2d461a429086c214c367743bb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"70284b7ca1924159b2904b45e5269437":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6d70a7e87f604e23bfd9ffd1e8071bad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e87ceec4df084261a92a51944956520d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"65ba0538ab62417cb8a73ca950514fcf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d80a7d252e7c48fc991c327df68a789a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"999956a376514fb891434eb152f8bfc5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5f506bdf974643a6973f0de8713531d7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ebf0805e46254dd9b7db39bdf0e74f82","IPY_MODEL_02762cfa95bc4266a429fbc9a1517056","IPY_MODEL_d799cf06d41d4a709ebdce2a574e3638"]}},"5f506bdf974643a6973f0de8713531d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ebf0805e46254dd9b7db39bdf0e74f82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f30130ae106a46aba1d4601ea1190ed2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ddf1ab4d16ec4d67924fa5b9157aa43a"}},"02762cfa95bc4266a429fbc9a1517056":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_12fc5045070a4e10a7e10a629ac901ac","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":481,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":481,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5869c4a4d59048d28745e0dfd3a797da"}},"d799cf06d41d4a709ebdce2a574e3638":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_63d46a4c93624180b9356a735be68c40","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 481/481 [00:00&lt;00:00, 15.6kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ca7e12a461eb4803a642fc39cdaecba8"}},"f30130ae106a46aba1d4601ea1190ed2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ddf1ab4d16ec4d67924fa5b9157aa43a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"12fc5045070a4e10a7e10a629ac901ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5869c4a4d59048d28745e0dfd3a797da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"63d46a4c93624180b9356a735be68c40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ca7e12a461eb4803a642fc39cdaecba8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a156633a159c493b8e766c2c25634526":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8d294ad67fcc4832b0d5378c6c791183","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3321adfbf4624e5897187d253eca9076","IPY_MODEL_21813d20501f4203b607498bbb239926","IPY_MODEL_cbccb15c186c48a3900267e189f03a5b"]}},"8d294ad67fcc4832b0d5378c6c791183":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3321adfbf4624e5897187d253eca9076":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9faf99e0387344fdb189406416e00d4a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_edf46d0b8a874d25bc5c353786cd21d9"}},"21813d20501f4203b607498bbb239926":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1504b50feb5548aeaaf08fda991a8441","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":501200538,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":501200538,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_981c1f3a73dd4fb6bd1ce510c50d1eb1"}},"cbccb15c186c48a3900267e189f03a5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9911f849adc64668a13d39f7b03c1fd7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 501M/501M [00:11&lt;00:00, 44.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9a4cac477db14c7a960cd25b2e151c39"}},"9faf99e0387344fdb189406416e00d4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"edf46d0b8a874d25bc5c353786cd21d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1504b50feb5548aeaaf08fda991a8441":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"981c1f3a73dd4fb6bd1ce510c50d1eb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9911f849adc64668a13d39f7b03c1fd7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9a4cac477db14c7a960cd25b2e151c39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pp3_2ZLq6NS","executionInfo":{"status":"ok","timestamp":1631143397629,"user_tz":420,"elapsed":13167,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"9f72f5ad-df68-472a-89d4-6223bdb74b10"},"source":["from datetime import datetime\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import re, string\n","import time\n","import random\n","import gc\n","import os, sys\n","import json\n","from copy import deepcopy\n","\n","#####\n","# !pip install gdown\n","!pip install transformers\n","pd.options.display.max_colwidth = 500\n","#####\n","\n","import torch\n","\n","from transformers import BertForSequenceClassification, AdamW , BertConfig, BertModel\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import LineByLineTextDataset\n","\n","from torch.nn import LSTM, Dropout, Linear, BCELoss, CrossEntropyLoss, Softmax, ReLU, LeakyReLU, Tanh\n","from torch.nn.functional import softmax, relu\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.utils.data.dataset import Subset\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n","\n","%load_ext autoreload \n","%autoreload 2"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n","\u001b[K     |████████████████████████████████| 2.8 MB 7.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 47.4 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 52.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 20.7 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Collecting huggingface-hub>=0.0.12\n","  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 5.5 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzEvkGACWMqX","executionInfo":{"status":"ok","timestamp":1631143432249,"user_tz":420,"elapsed":34632,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"cc972932-912e-49b2-da7b-3c1a892e6b0e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUYifLXWu1D0","executionInfo":{"status":"ok","timestamp":1631143432857,"user_tz":420,"elapsed":613,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"6bdfef9b-7ed3-48e2-e217-5aa1a7df28ff"},"source":["!ls \"/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model\""],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["'Bert Models'   models.py   __pycache__   RoBerta  'Roberta Final'\n"]}]},{"cell_type":"code","metadata":{"id":"f6srHxsX4FhM","executionInfo":{"status":"ok","timestamp":1631143432857,"user_tz":420,"elapsed":4,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":["PATH_PREFIX = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8keHP8s6Fj4","executionInfo":{"status":"ok","timestamp":1631143435688,"user_tz":420,"elapsed":2834,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":["sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils')\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model')\n","\n","from models import *\n","from text_processing import *"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFayi-JvrDHx","executionInfo":{"status":"ok","timestamp":1631143438661,"user_tz":420,"elapsed":2977,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"2429afb2-5f80-463c-c7a8-20e7797bf2f1"},"source":["# Enable GPU before proceeding\n","device_name = tf.test.gpu_device_name()\n","print(\"Device found:\", device_name)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Device found: /device:GPU:0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLkb-vZhryC-","executionInfo":{"status":"ok","timestamp":1631143438662,"user_tz":420,"elapsed":18,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"13dfadcb-07bd-40ff-9170-32bca3b0e7b5"},"source":["# Test if CUDA is available, use it\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","print(\"Using CUDA Device:\", torch.cuda.get_device_name())"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA Device: Tesla T4\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jgmig-gyT5_U"},"source":["# DOWNLOAD AND PREPROCESS DATA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIGMfizwRlrM","executionInfo":{"status":"ok","timestamp":1631143439601,"user_tz":420,"elapsed":954,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"ee3d2ce3-f271-4f7a-a43a-765f0716fe03"},"source":["train_dt, test_dt = load_data(PATH_PREFIX + '/Data/HER_final_data.csv', \n","                   dtype_dict={'tweetid':str, 'label':int, 'target':int, 'aggr':int})"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["label\n","0    1894\n","1     304\n","2     202\n","Name: label, dtype: int64\n","target\n","0    2003\n","1     367\n","2      25\n","3       5\n","Name: target, dtype: int64\n","aggr\n","0    1799\n","1     533\n","2      68\n","Name: aggr, dtype: int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"42nIFpyRNTJw"},"source":["## Tokenize state"]},{"cell_type":"code","metadata":{"id":"GPIccBVB_yr1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631143439908,"user_tz":420,"elapsed":310,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"a22d04eb-b8c9-4b66-b8dd-dcd940508ef5"},"source":["# import hashtag_dict \n","HASHTAG_DIR = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils/' \n","with open(HASHTAG_DIR + 'hashtag_dict.json', 'r') as f:\n","    hashtag_dict = json.load(f)\n","\n","# CUDA is memory hungry. Needs to reduce the max_len\n","MAX_LEN = 225\n","print(\"Maximum length of cleaned up dataset is:\", MAX_LEN)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum length of cleaned up dataset is: 225\n"]}]},{"cell_type":"code","metadata":{"id":"CIPViSOuXhTA","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["ae1b868d52ee40bd99b6aab8c96b0ef9","4f2641babf634cf7b7980ab8acad6c69","cca313ab39e5477cae4c7dffd33a198a","6e26f04c4f39472d9af38f8e9f0a8e26","2131435212dc4a6181a69326cdb61b8e","812d982eefae418ea3ef0c089e05ebf9","f9ddc109af934b6aafba23a4370f7bcf","6c96f89a1b0a453b98159171d7670ed4","38f1acfacdc74089ac4ad100364ba6c0","703838841767410ba541c19e1b9295a9","8e6c2cba0c9e4a54aeda0bbe40ec98f3","a0801124e4804edc8e7df8279bd31dd1","937e1e12c4e646679e42ad1f46f3c941","5c8159c8fcfa4a8d8fde225386243d89","9c126273f226451ab0713c5b76bddbc9","a6d99667508e4b2889eb44464e693fb6","530869a160884a09941a87cd4349b809","ff61733887854fe893bdb53165d5fdf8","36751a2791e64c9d9aa65c08c732656c","1e7ef4395521496f907711635a3a7f79","b7c74e6088db4a6fa5fe740b4f55139b","b69637095d88496ebd5490e2f89ce981","2190ae2583d146138f46ca84bc43923d","51d0a9dfd0d747c48854f17c3b4627ec","26ffd5c2a4244a10a77c6dfb6325364a","e9d1c6a93b7648d3a3ec95674d823869","d0ee4f72c1ba42238b10f7866781a98a","d9023b2d461a429086c214c367743bb4","70284b7ca1924159b2904b45e5269437","6d70a7e87f604e23bfd9ffd1e8071bad","e87ceec4df084261a92a51944956520d","65ba0538ab62417cb8a73ca950514fcf","d80a7d252e7c48fc991c327df68a789a","999956a376514fb891434eb152f8bfc5","5f506bdf974643a6973f0de8713531d7","ebf0805e46254dd9b7db39bdf0e74f82","02762cfa95bc4266a429fbc9a1517056","d799cf06d41d4a709ebdce2a574e3638","f30130ae106a46aba1d4601ea1190ed2","ddf1ab4d16ec4d67924fa5b9157aa43a","12fc5045070a4e10a7e10a629ac901ac","5869c4a4d59048d28745e0dfd3a797da","63d46a4c93624180b9356a735be68c40","ca7e12a461eb4803a642fc39cdaecba8"]},"executionInfo":{"status":"ok","timestamp":1631143444831,"user_tz":420,"elapsed":4926,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"f292e4f3-f740-40e9-bb5a-16ed03b941c7"},"source":["tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae1b868d52ee40bd99b6aab8c96b0ef9","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a0801124e4804edc8e7df8279bd31dd1","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2190ae2583d146138f46ca84bc43923d","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"999956a376514fb891434eb152f8bfc5","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HP_qSMkuohrO","executionInfo":{"status":"ok","timestamp":1631143446700,"user_tz":420,"elapsed":1872,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"03f3f311-426e-40c1-a8a5-7f7d9c426d04"},"source":["train_input_ids, train_attn_masks  = tokenizing(train_dt.text.values, tokenizer, MAX_LEN)\n","test_input_ids, test_attn_masks  = tokenizing(test_dt.text.values, tokenizer, MAX_LEN)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Total time taken:  1.5272228717803955\n","Total time taken:  0.3076448440551758\n"]}]},{"cell_type":"code","metadata":{"id":"5UaN8FNEUbn9","executionInfo":{"status":"ok","timestamp":1631143446818,"user_tz":420,"elapsed":122,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":["varlist = ['label', 'target', 'aggr']\n","_, train_tensor = arrange_tensor(train_dt, train_input_ids, train_attn_masks ,varlist=varlist)\n","_, test_tensor  = arrange_tensor(test_dt, test_input_ids, test_attn_masks ,varlist=varlist)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RNJyuEsVfCB","executionInfo":{"status":"ok","timestamp":1631143446936,"user_tz":420,"elapsed":121,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"b90cddee-4d8a-4455-9c02-8c2e8592d823"},"source":["test_tensor[:][-1].shape"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([600])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"jKEuQnyKzKaP"},"source":["# BUILD MODEL"]},{"cell_type":"markdown","metadata":{"id":"ptpLoOHEKWBx"},"source":["In this model, we will build an LSTM model leveraging BERT base hidden data on top. The key difference compared to simpleBert is that we need to save the parameters."]},{"cell_type":"code","metadata":{"id":"t0NtCmP8NM8B","executionInfo":{"status":"ok","timestamp":1631143446937,"user_tz":420,"elapsed":4,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":["SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Saved Parameters/\""],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"2K9sRD_qgCEY","executionInfo":{"status":"ok","timestamp":1631143447070,"user_tz":420,"elapsed":136,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":["def run_models(save_path:str, model_class, num_labels, num_tasks, max_len,\n","        num_epochs, loss_funcs:list, device,\n","        train_tensor, test_tensor, val_tensor=None, batch_size=20, label_ls = [[0,1,2]],\n","        seed_list = [101,211,307,401,503], save_seed=0,\n","        scheduler=get_linear_schedule_with_warmup):\n","\n","    final_metrics = dict()\n","    # Iterate through seeds\n","    for idx, s in enumerate(seed_list):\n","        print('========== Trying seed', s, '==========')\n","        # Reset the seed\n","        seed = s\n","        random.seed(seed)\n","        os.environ['PYTHONHASHSEED'] = str(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = True\n","    \n","        # print(\"Random num: \", random.randint(1,1000))\n","        \n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        # Initialize model\n","        model = model_class('roberta-base', num_labels=num_labels, max_len=max_len)\n","        optimizer = AdamW(model.parameters(),lr=2e-5, eps=1e-6, weight_decay=1e-3)\n","        build_params = {'device':device, 'optimizer': optimizer, 'loss_functions': loss_funcs}\n","        # build \n","        model.build(**build_params)\n","\n","        # renew scheduler\n","        total_steps = train_tensor[:][-1].shape[0] * num_epochs\n","        scheduler_new = scheduler(model.opt, \n","                                  num_warmup_steps = 0, \n","                                  num_training_steps = total_steps)\n","        # train\n","        if val_tensor is None:\n","            model.train(train_tensor, batch_size = batch_size,\n","                        num_epochs=num_epochs, verbose=True, scheduler = scheduler_new)\n","        else: \n","            model.train(train_tensor, val_tensor=val_tensor, batch_size=batch_size,\n","                        num_epochs=num_epochs, verbose=True, scheduler = scheduler_new)\n","\n","        # evaluate\n","        _, logits, labels = model.evaluate(test_tensor)\n","        logits, labels = format_outputs(logits, labels, num_tasks)\n","        preds = get_labels_from_multi_logits(logits)\n","        res = print_report_multitask(labels, preds, label_ls )\n","\n","        final_metrics[s] = res\n","\n","        if idx==save_seed:\n","            model.save(save_path)\n","        del model\n","\n","    return final_metrics "],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a156633a159c493b8e766c2c25634526","8d294ad67fcc4832b0d5378c6c791183","3321adfbf4624e5897187d253eca9076","21813d20501f4203b607498bbb239926","cbccb15c186c48a3900267e189f03a5b","9faf99e0387344fdb189406416e00d4a","edf46d0b8a874d25bc5c353786cd21d9","1504b50feb5548aeaaf08fda991a8441","981c1f3a73dd4fb6bd1ce510c50d1eb1","9911f849adc64668a13d39f7b03c1fd7","9a4cac477db14c7a960cd25b2e151c39"]},"id":"_iTm811coQuf","executionInfo":{"status":"ok","timestamp":1631146299623,"user_tz":420,"elapsed":2852555,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"f2a71f23-3396-4319-8307-88161edbd282"},"source":["###### !!CHECK THIS!! ######\n","model_class = RobertaLSTM\n","model_str = 'Roberta_LSTM'\n","###########################\n","target_ls = ['label','target','aggr']\n","num_labels =  [3,4,3]\n","num_tasks = len(num_labels)\n","label_ls = [[0,1,2],[0,1,2,3],[0,1,2]]\n","num_epochs = 5\n","# change correct weights\n","weight_dict = calculate_class_weights(train_dt, target_ls)\n","weight_ls  = [list(weight_dict[k].values()) for k in target_ls]\n","loss_funcs = [CrossEntropyLoss(weight=torch.tensor(w).to(device)) for w in weight_ls]\n","\n","model_params = {'num_labels':num_labels, 'num_tasks':num_tasks, \n","                'max_len':MAX_LEN,'label_ls':label_ls,\n","                'num_epochs':num_epochs, 'loss_funcs':loss_funcs,\n","                }\n","\n","results = run_models(save_path = SAVE_PATH, model_class=model_class, device=device, \n","     train_tensor=train_tensor, test_tensor=test_tensor, \n","    #  seed_list=[101,211],\n","      **model_params )"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["========== Trying seed 101 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a156633a159c493b8e766c2c25634526","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [112.81621566414833, 118.1646559536457, 113.76614388823509]. Validation loss: []. Duration: 97.90632176399231\n","Running Epoch  1\n","Training loss : [91.14361789822578, 80.07675881683826, 80.54531854391098]. Validation loss: []. Duration: 104.76760649681091\n","Running Epoch  2\n","Training loss : [78.58256913721561, 63.579342629760504, 65.56525307893753]. Validation loss: []. Duration: 110.7093026638031\n","Running Epoch  3\n","Training loss : [69.90613304078579, 47.85499774105847, 56.366380490362644]. Validation loss: []. Duration: 111.37611293792725\n","Running Epoch  4\n","Training loss : [63.86922971904278, 36.53650429658592, 38.27203429862857]. Validation loss: []. Duration: 111.09857249259949\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.92016807 0.40963855 0.41463415]\n","recall   : [0.90309278 0.43589744 0.45945946]\n","fscore   : [0.91155047 0.42236025 0.43589744]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.94262295 0.68888889 0.33333333 0.        ]\n","recall   : [0.91633466 0.71264368 0.7        0.        ]\n","fscore   : [0.92929293 0.70056497 0.4516129  0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.88435374 0.58278146 0.5       ]\n","recall   : [0.88235294 0.63768116 0.2       ]\n","fscore   : [0.88335221 0.60899654 0.28571429]\n","support  : [442 138  20]\n","========== Trying seed 211 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [113.98499074578285, 118.27294689416885, 112.60030817985535]. Validation loss: []. Duration: 111.49422311782837\n","Running Epoch  1\n","Training loss : [86.03897747397423, 78.50068426132202, 78.57710100710392]. Validation loss: []. Duration: 111.44425392150879\n","Running Epoch  2\n","Training loss : [77.91999733448029, 58.30777836591005, 66.75045439600945]. Validation loss: []. Duration: 111.86739444732666\n","Running Epoch  3\n","Training loss : [67.79802567511797, 38.35061002150178, 52.279807180166245]. Validation loss: []. Duration: 111.37567615509033\n","Running Epoch  4\n","Training loss : [63.35845968872309, 33.947966231033206, 46.54479556530714]. Validation loss: []. Duration: 111.39499950408936\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.94343891 0.37974684 0.30379747]\n","recall   : [0.85979381 0.38461538 0.64864865]\n","fscore   : [0.89967638 0.38216561 0.4137931 ]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.96452328 0.56934307 0.16666667 0.        ]\n","recall   : [0.86653386 0.89655172 0.2        0.        ]\n","fscore   : [0.91290661 0.69642857 0.18181818 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.90625    0.55555556 0.30769231]\n","recall   : [0.85294118 0.6884058  0.2       ]\n","fscore   : [0.87878788 0.61488673 0.24242424]\n","support  : [442 138  20]\n","========== Trying seed 307 ==========\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [110.75891935825348, 116.60368165373802, 110.80886369943619]. Validation loss: []. Duration: 111.23810267448425\n","Running Epoch  1\n","Training loss : [87.20782071352005, 78.2120352089405, 82.93673062324524]. Validation loss: []. Duration: 111.74653363227844\n","Running Epoch  2\n","Training loss : [76.47848139703274, 59.357988715171814, 66.28118558228016]. Validation loss: []. Duration: 111.36957168579102\n","Running Epoch  3\n","Training loss : [69.22585500776768, 50.41544906049967, 53.96535239368677]. Validation loss: []. Duration: 111.11671352386475\n","Running Epoch  4\n","Training loss : [61.779628306627274, 35.0856011454016, 43.27882731705904]. Validation loss: []. Duration: 111.08324241638184\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.90631365 0.44       0.41176471]\n","recall   : [0.91752577 0.42307692 0.37837838]\n","fscore   : [0.91188525 0.43137255 0.3943662 ]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.93814433 0.62376238 0.21428571 0.        ]\n","recall   : [0.9063745  0.72413793 0.3        0.        ]\n","fscore   : [0.92198582 0.67021277 0.25       0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.87581699 0.6        0.5       ]\n","recall   : [0.90950226 0.58695652 0.15      ]\n","fscore   : [0.89234184 0.59340659 0.23076923]\n","support  : [442 138  20]\n","========== Trying seed 401 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [113.22202935814857, 114.23940770328045, 110.7419955432415]. Validation loss: []. Duration: 111.37638235092163\n","Running Epoch  1\n","Training loss : [87.64581529796124, 78.5638547167182, 82.07905530929565]. Validation loss: []. Duration: 111.27838706970215\n","Running Epoch  2\n","Training loss : [74.84278036653996, 57.142729464918375, 59.14568282663822]. Validation loss: []. Duration: 111.8616898059845\n","Running Epoch  3\n","Training loss : [68.337640799582, 46.40206852555275, 49.866137839853764]. Validation loss: []. Duration: 111.37641167640686\n","Running Epoch  4\n","Training loss : [65.04681057482958, 36.18856011889875, 39.99307287298143]. Validation loss: []. Duration: 111.39114093780518\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.95774648 0.36065574 0.26548673]\n","recall   : [0.84123711 0.28205128 0.81081081]\n","fscore   : [0.89571899 0.31654676 0.4       ]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.96736597 0.56737589 0.06666667 0.        ]\n","recall   : [0.82669323 0.91954023 0.2        0.        ]\n","fscore   : [0.8915145  0.70175439 0.1        0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.89906103 0.56       0.33333333]\n","recall   : [0.86651584 0.60869565 0.4       ]\n","fscore   : [0.88248848 0.58333333 0.36363636]\n","support  : [442 138  20]\n","========== Trying seed 503 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [113.53068614006042, 116.36501002311707, 112.91253206133842]. Validation loss: []. Duration: 111.61270117759705\n","Running Epoch  1\n","Training loss : [87.42059287428856, 75.64369738101959, 81.30568423867226]. Validation loss: []. Duration: 111.0992579460144\n","Running Epoch  2\n","Training loss : [74.96859292685986, 57.39006406441331, 61.30712468922138]. Validation loss: []. Duration: 111.56481909751892\n","Running Epoch  3\n","Training loss : [68.45710515230894, 41.13001112267375, 50.237154837697744]. Validation loss: []. Duration: 111.74163579940796\n","Running Epoch  4\n","Training loss : [62.002332642674446, 34.1654269406572, 44.36528524570167]. Validation loss: []. Duration: 111.33595752716064\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.9279661  0.49180328 0.31343284]\n","recall   : [0.90309278 0.38461538 0.56756757]\n","fscore   : [0.9153605  0.43165468 0.40384615]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.95493562 0.62280702 0.10526316 0.        ]\n","recall   : [0.88645418 0.81609195 0.2        0.        ]\n","fscore   : [0.91942149 0.70646766 0.13793103 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.88274336 0.60144928 0.4       ]\n","recall   : [0.90271493 0.60144928 0.2       ]\n","fscore   : [0.89261745 0.60144928 0.26666667]\n","support  : [442 138  20]\n"]}]},{"cell_type":"code","metadata":{"id":"HPpGBDuFRMcl","executionInfo":{"status":"ok","timestamp":1631146299625,"user_tz":420,"elapsed":11,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":["RESULT_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Results/\"\n","\n","specific_res = get_specific_results(results)\n","specific_res.to_pickle(RESULT_PATH + model_str + '_specific.pkl')\n","\n","macro_res    = get_macro_results(results)\n","macro_res.to_pickle(RESULT_PATH + model_str + '_macro.pkl')"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3V4slhXyCB_","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1631146299805,"user_tz":420,"elapsed":187,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"d0d9d22c-1673-435f-fc19-0c4e46b5d931"},"source":["specific_res"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Normal</th>\n","      <th>Abusive</th>\n","      <th>Hate</th>\n","      <th>Neither/NA</th>\n","      <th>Anti-Asian</th>\n","      <th>Anti-Black</th>\n","      <th>Both</th>\n","      <th>No.Agg.</th>\n","      <th>Some.Agg.</th>\n","      <th>Very.Agg.</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.93 +- 0.02</td>\n","      <td>0.42 +- 0.05</td>\n","      <td>0.34 +- 0.06</td>\n","      <td>0.95 +- 0.01</td>\n","      <td>0.61 +- 0.04</td>\n","      <td>0.18 +- 0.09</td>\n","      <td>--</td>\n","      <td>0.89 +- 0.01</td>\n","      <td>0.58 +- 0.02</td>\n","      <td>0.41 +- 0.08</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.88 +- 0.03</td>\n","      <td>0.38 +- 0.05</td>\n","      <td>0.57 +- 0.15</td>\n","      <td>0.88 +- 0.03</td>\n","      <td>0.81 +- 0.09</td>\n","      <td>0.32 +- 0.19</td>\n","      <td>--</td>\n","      <td>0.88 +- 0.02</td>\n","      <td>0.62 +- 0.04</td>\n","      <td>0.23 +- 0.09</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.91 +- 0.01</td>\n","      <td>0.4 +- 0.04</td>\n","      <td>0.41 +- 0.01</td>\n","      <td>0.92 +- 0.01</td>\n","      <td>0.7 +- 0.01</td>\n","      <td>0.22 +- 0.12</td>\n","      <td>--</td>\n","      <td>0.89 +- 0.01</td>\n","      <td>0.6 +- 0.01</td>\n","      <td>0.28 +- 0.05</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Normal       Abusive  ...     Some.Agg.     Very.Agg.\n","0  0.93 +- 0.02  0.42 +- 0.05  ...  0.58 +- 0.02  0.41 +- 0.08\n","1  0.88 +- 0.03  0.38 +- 0.05  ...  0.62 +- 0.04  0.23 +- 0.09\n","2  0.91 +- 0.01   0.4 +- 0.04  ...   0.6 +- 0.01  0.28 +- 0.05\n","\n","[3 rows x 10 columns]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"BbUQOvoq78Ru","colab":{"base_uri":"https://localhost:8080/","height":80},"executionInfo":{"status":"ok","timestamp":1631146299807,"user_tz":420,"elapsed":11,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}},"outputId":"4f629ada-2189-42ce-8770-d5529d33c07b"},"source":["macro_res"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.56 +- 0.02</td>\n","      <td>0.61 +- 0.03</td>\n","      <td>0.59 +- 0.01</td>\n","      <td>0.78 +- 0.02</td>\n","      <td>0.85 +- 0.03</td>\n","      <td>0.81 +- 0.01</td>\n","      <td>0.63 +- 0.03</td>\n","      <td>0.58 +- 0.03</td>\n","      <td>0.6 +- 0.01</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              0             1  ...             7            8\n","0  0.56 +- 0.02  0.61 +- 0.03  ...  0.58 +- 0.03  0.6 +- 0.01\n","\n","[1 rows x 9 columns]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"4pfEs3OfRqRJ","executionInfo":{"status":"ok","timestamp":1631146299808,"user_tz":420,"elapsed":11,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"13846557879214077272"}}},"source":[""],"execution_count":19,"outputs":[]}]}