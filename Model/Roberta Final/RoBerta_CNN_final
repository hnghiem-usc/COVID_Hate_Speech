{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBerta_CNN_final","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pp3_2ZLq6NS","executionInfo":{"status":"ok","timestamp":1631120474970,"user_tz":420,"elapsed":6929,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"1bfb7eae-bbf0-49e2-9345-2274aace5b1f"},"source":["from datetime import datetime\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import re, string\n","import time\n","import random\n","import gc\n","import os, sys\n","import json\n","from copy import deepcopy\n","\n","#####\n","# !pip install gdown\n","!pip install transformers\n","pd.options.display.max_colwidth = 500\n","#####\n","\n","import torch\n","\n","from transformers import BertForSequenceClassification, AdamW , BertConfig, BertModel\n","from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import LineByLineTextDataset\n","\n","from torch.nn import LSTM, Dropout, Linear, BCELoss, CrossEntropyLoss, Softmax, ReLU, LeakyReLU, Tanh\n","from torch.nn.functional import softmax, relu\n","from torch.nn.utils.rnn import pack_padded_sequence\n","from torch.utils.data.dataset import Subset\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n","\n","%load_ext autoreload \n","%autoreload 2"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MzEvkGACWMqX","executionInfo":{"status":"ok","timestamp":1631120475097,"user_tz":420,"elapsed":133,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"9c41fdab-f926-4973-db7a-086879bcd3eb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUYifLXWu1D0","executionInfo":{"status":"ok","timestamp":1631120475189,"user_tz":420,"elapsed":93,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"2bf499a3-76ae-45b2-9584-1f6feb3c54f5"},"source":["!ls \"/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'Bert Models'   models.py   __pycache__   RoBerta  'Roberta Final'\n"]}]},{"cell_type":"code","metadata":{"id":"f6srHxsX4FhM"},"source":["PATH_PREFIX = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8keHP8s6Fj4"},"source":["sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils')\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/Model')\n","\n","from models import *\n","from text_processing import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFayi-JvrDHx","executionInfo":{"status":"ok","timestamp":1631120476649,"user_tz":420,"elapsed":148,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"13d13b9d-7740-45f2-e835-ec61e512a325"},"source":["# Enable GPU before proceeding\n","device_name = tf.test.gpu_device_name()\n","print(\"Device found:\", device_name)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device found: /device:GPU:0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLkb-vZhryC-","executionInfo":{"status":"ok","timestamp":1631120476729,"user_tz":420,"elapsed":81,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"d47919e3-84b8-46ea-f1d6-899a08c0f58b"},"source":["# Test if CUDA is available, use it\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","print(\"Using CUDA Device:\", torch.cuda.get_device_name())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using CUDA Device: Tesla K80\n"]}]},{"cell_type":"markdown","metadata":{"id":"Jgmig-gyT5_U"},"source":["# DOWNLOAD AND PREPROCESS DATA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TIGMfizwRlrM","executionInfo":{"status":"ok","timestamp":1631120476822,"user_tz":420,"elapsed":94,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"2b0942d3-6009-47a1-b9a4-93fd2301362e"},"source":["train_dt, test_dt = load_data(PATH_PREFIX + '/Data/HER_final_data.csv', \n","                   dtype_dict={'tweetid':str, 'label':int, 'target':int, 'aggr':int})"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["label\n","0    1894\n","1     304\n","2     202\n","Name: label, dtype: int64\n","target\n","0    2003\n","1     367\n","2      25\n","3       5\n","Name: target, dtype: int64\n","aggr\n","0    1799\n","1     533\n","2      68\n","Name: aggr, dtype: int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"42nIFpyRNTJw"},"source":["## Tokenize state"]},{"cell_type":"code","metadata":{"id":"GPIccBVB_yr1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631120476940,"user_tz":420,"elapsed":120,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"c1e3446f-b3da-439b-d695-50cb16fdf68c"},"source":["# import hashtag_dict \n","HASHTAG_DIR = '/content/drive/MyDrive/Colab Notebooks/Bert Hate Speech/utils/' \n","with open(HASHTAG_DIR + 'hashtag_dict.json', 'r') as f:\n","    hashtag_dict = json.load(f)\n","\n","# CUDA is memory hungry. Needs to reduce the max_len\n","MAX_LEN = 225\n","print(\"Maximum length of cleaned up dataset is:\", MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum length of cleaned up dataset is: 225\n"]}]},{"cell_type":"code","metadata":{"id":"CIPViSOuXhTA"},"source":["tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HP_qSMkuohrO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631120480935,"user_tz":420,"elapsed":2370,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"81906508-ef75-4109-adc1-c882f01aa72c"},"source":["train_input_ids, train_attn_masks  = tokenizing(train_dt.text.values, tokenizer, MAX_LEN)\n","test_input_ids, test_attn_masks  = tokenizing(test_dt.text.values, tokenizer, MAX_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Total time taken:  1.8780155181884766\n","Total time taken:  0.4094970226287842\n"]}]},{"cell_type":"code","metadata":{"id":"5UaN8FNEUbn9"},"source":["varlist = ['label', 'target', 'aggr']\n","_, train_tensor = arrange_tensor(train_dt, train_input_ids, train_attn_masks ,varlist=varlist)\n","_, test_tensor  = arrange_tensor(test_dt, test_input_ids, test_attn_masks ,varlist=varlist)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8RNJyuEsVfCB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631120481090,"user_tz":420,"elapsed":8,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"e47aeb91-81e1-4f4c-c3b1-a14a2f210078"},"source":["test_tensor[:][-1].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([600])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"jKEuQnyKzKaP"},"source":["# BUILD MODEL"]},{"cell_type":"markdown","metadata":{"id":"ptpLoOHEKWBx"},"source":["In this model, we will build an LSTM model leveraging BERT base hidden data on top. The key difference compared to simpleBert is that we need to save the parameters."]},{"cell_type":"code","metadata":{"id":"t0NtCmP8NM8B"},"source":["SAVE_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Saved Parameters/Roberta\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2K9sRD_qgCEY"},"source":["def run_models(save_path:str, model_class, num_labels, num_tasks, max_len,\n","        num_epochs, loss_funcs:list, device,\n","        train_tensor, test_tensor, val_tensor=None, batch_size=20, label_ls = [[0,1,2]],\n","        seed_list = [101,211,307,401,503], save_seed=0):\n","\n","    final_metrics = dict()\n","    # Iterate through seeds\n","    for idx, s in enumerate(seed_list):\n","        print('========== Trying seed', s, '==========')\n","        # Reset the seed\n","        seed = s\n","        random.seed(seed)\n","        os.environ['PYTHONHASHSEED'] = str(seed)\n","        np.random.seed(seed)\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = True\n","    \n","        # print(\"Random num: \", random.randint(1,1000))\n","        \n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        # Initialize model\n","        model = model_class('roberta-base', num_labels=num_labels, max_len=max_len)\n","        optimizer = AdamW(model.parameters(),lr=2e-5, eps=1e-6, weight_decay=1e-3)\n","        build_params = {'device':device, 'optimizer': optimizer, 'loss_functions': loss_funcs}\n","        # build \n","        model.build(**build_params)\n","\n","        # train\n","        if val_tensor is None:\n","            model.train(train_tensor, batch_size = batch_size,\n","                        num_epochs=num_epochs, verbose=True)\n","        else: \n","            model.train(train_tensor, val_tensor=val_tensor, batch_size=batch_size,\n","                        num_epochs=num_epochs, verbose=True)\n","\n","        # evaluate\n","        _, logits, labels = model.evaluate(test_tensor)\n","        logits, labels = format_outputs(logits, labels, num_tasks)\n","        preds = get_labels_from_multi_logits(logits)\n","        res = print_report_multitask(labels, preds, label_ls )\n","\n","        final_metrics[s] = res\n","\n","        if idx==save_seed:\n","            model.save(save_path)\n","        del model\n","\n","    return final_metrics "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_iTm811coQuf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631126480937,"user_tz":420,"elapsed":5999674,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"844d21dc-2a74-4991-ead5-912690f2027c"},"source":["###### !!CHECK THIS!! ######\n","model_class = RobertaCNN\n","model_str = 'Roberta_CNN'\n","###########################\n","target_ls = ['label','target','aggr']\n","num_labels =  [3,4,3]\n","num_tasks = len(num_labels)\n","label_ls = [[0,1,2],[0,1,2,3],[0,1,2]]\n","num_epochs = 5\n","\n","batch_size = 15\n","# change correct weights\n","weight_dict = calculate_class_weights(train_dt, target_ls)\n","weight_ls  = [list(weight_dict[k].values()) for k in target_ls]\n","loss_funcs = [CrossEntropyLoss(weight=torch.tensor(w).to(device)) for w in weight_ls]\n","\n","model_params = {'num_labels':num_labels, 'num_tasks':num_tasks, \n","                'max_len':MAX_LEN,'label_ls':label_ls,\n","                'num_epochs':num_epochs, 'loss_funcs':loss_funcs,\n","                'batch_size':batch_size\n","                }\n","\n","results = run_models(save_path = SAVE_PATH, model_class=model_class, device=device, \n","     train_tensor=train_tensor, test_tensor=test_tensor, \n","    #  seed_list=[101,211],\n","      **model_params )"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["========== Trying seed 101 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"output_type":"stream","name":"stdout","text":["Training loss : [232.74445170164108, 254.71146649122238, 250.13575917482376]. Validation loss: []. Duration: 234.4452884197235\n","Running Epoch  1\n","Training loss : [147.19784697145224, 153.93442951142788, 139.30701234191656]. Validation loss: []. Duration: 233.7134473323822\n","Running Epoch  2\n","Training loss : [161.64802591502666, 170.30310842767358, 147.49846279621124]. Validation loss: []. Duration: 233.55552458763123\n","Running Epoch  3\n","Training loss : [123.48691394086927, 125.2360437256284, 122.68821166828275]. Validation loss: []. Duration: 233.28438520431519\n","Running Epoch  4\n","Training loss : [117.30349773168564, 85.28189231082797, 102.75854826718569]. Validation loss: []. Duration: 232.13230967521667\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.94013304 0.39175258 0.36538462]\n","recall   : [0.8742268  0.48717949 0.51351351]\n","fscore   : [0.90598291 0.43428571 0.42696629]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.95758929 0.65740741 0.09090909 0.        ]\n","recall   : [0.85458167 0.81609195 0.4        0.        ]\n","fscore   : [0.90315789 0.72820513 0.14814815 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.90182648 0.61146497 0.8       ]\n","recall   : [0.89366516 0.69565217 0.2       ]\n","fscore   : [0.89772727 0.65084746 0.32      ]\n","support  : [442 138  20]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["========== Trying seed 211 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [295.74855667352676, 250.99713225662708, 220.92514425516129]. Validation loss: []. Duration: 235.18715739250183\n","Running Epoch  1\n","Training loss : [163.1217253357172, 165.45143021643162, 154.20888203382492]. Validation loss: []. Duration: 234.73114132881165\n","Running Epoch  2\n","Training loss : [141.67545330524445, 127.72048467025161, 143.95955961942673]. Validation loss: []. Duration: 234.6036114692688\n","Running Epoch  3\n","Training loss : [119.66216439008713, 118.94133971258998, 113.61677452921867]. Validation loss: []. Duration: 234.69564175605774\n","Running Epoch  4\n","Training loss : [117.17767426371574, 101.15573750808835, 115.05267888307571]. Validation loss: []. Duration: 234.8006727695465\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.93681917 0.425      0.31683168]\n","recall   : [0.88659794 0.21794872 0.86486486]\n","fscore   : [0.91101695 0.28813559 0.46376812]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.96983759 0.57777778 0.11764706 0.        ]\n","recall   : [0.83266932 0.89655172 0.4        0.        ]\n","fscore   : [0.8960343  0.7027027  0.18181818 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.90330189 0.56666667 0.38461538]\n","recall   : [0.86651584 0.61594203 0.5       ]\n","fscore   : [0.88452656 0.59027778 0.43478261]\n","support  : [442 138  20]\n","========== Trying seed 307 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [268.14486560225487, 242.6356171965599, 252.58542227745056]. Validation loss: []. Duration: 234.56421375274658\n","Running Epoch  1\n","Training loss : [152.32551091909409, 159.9812162220478, 152.3507555127144]. Validation loss: []. Duration: 234.62829637527466\n","Running Epoch  2\n","Training loss : [150.04091574251652, 149.44162312150002, 178.5530747026205]. Validation loss: []. Duration: 234.50526881217957\n","Running Epoch  3\n","Training loss : [134.5929237678647, 137.85968935117126, 152.32642133161426]. Validation loss: []. Duration: 234.5467448234558\n","Running Epoch  4\n","Training loss : [120.72142291488126, 128.3488950561732, 120.12651447951794]. Validation loss: []. Duration: 234.7162299156189\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.95673077 0.32941176 0.57142857]\n","recall   : [0.82061856 0.71794872 0.21621622]\n","fscore   : [0.88346282 0.4516129  0.31372549]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.96774194 0.67889908 0.05263158 0.        ]\n","recall   : [0.83665339 0.85057471 0.3        0.        ]\n","fscore   : [0.8974359  0.75510204 0.08955224 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.90330189 0.55244755 0.21212121]\n","recall   : [0.86651584 0.57246377 0.35      ]\n","fscore   : [0.88452656 0.56227758 0.26415094]\n","support  : [442 138  20]\n","========== Trying seed 401 ==========\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [286.5325213074684, 295.6772749722004, 352.71671855449677]. Validation loss: []. Duration: 234.5744171142578\n","Running Epoch  1\n","Training loss : [168.69888737797737, 205.04011759907007, 161.00935815274715]. Validation loss: []. Duration: 234.65960431098938\n","Running Epoch  2\n","Training loss : [174.04897940158844, 203.11877567134798, 164.35781372711062]. Validation loss: []. Duration: 234.56729125976562\n","Running Epoch  3\n","Training loss : [133.35612452030182, 162.3412286862731, 143.25659495592117]. Validation loss: []. Duration: 234.61821365356445\n","Running Epoch  4\n","Training loss : [147.2523761242628, 157.75773398950696, 138.3947592265904]. Validation loss: []. Duration: 234.5931589603424\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.93304536 0.35849057 0.30952381]\n","recall   : [0.89072165 0.24358974 0.7027027 ]\n","fscore   : [0.91139241 0.29007634 0.42975207]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.96674058 0.6        0.10526316 0.        ]\n","recall   : [0.8685259  0.89655172 0.2        0.        ]\n","fscore   : [0.91500525 0.71889401 0.13793103 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.87418655 0.60330579 0.27777778]\n","recall   : [0.91176471 0.52898551 0.25      ]\n","fscore   : [0.89258029 0.56370656 0.26315789]\n","support  : [442 138  20]\n","========== Trying seed 503 ==========\n","Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Multitask training: 3 tasks \n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Running Epoch  0\n","Training loss : [265.06250005960464, 247.5531602203846, 252.58662275969982]. Validation loss: []. Duration: 234.42205214500427\n","Running Epoch  1\n","Training loss : [157.40554174780846, 152.03648269176483, 160.87442530691624]. Validation loss: []. Duration: 234.59417271614075\n","Running Epoch  2\n","Training loss : [156.7158735692501, 142.27412270754576, 149.96333990991116]. Validation loss: []. Duration: 234.54529905319214\n","Running Epoch  3\n","Training loss : [144.35687732696533, 154.6216147094965, 150.5477515310049]. Validation loss: []. Duration: 234.46832990646362\n","Running Epoch  4\n","Training loss : [127.98642794787884, 120.65667339786887, 106.40691258013248]. Validation loss: []. Duration: 234.63098549842834\n","num_tasks 3\n","\n","-----Report for task 0:-----\n","\n","precision: [0.94847775 0.3381295  0.44117647]\n","recall   : [0.83505155 0.6025641  0.40540541]\n","fscore   : [0.88815789 0.43317972 0.42253521]\n","support  : [485  78  37]\n","\n","-----Report for task 1:-----\n","\n","precision: [0.93983402 0.67032967 0.11111111 0.        ]\n","recall   : [0.90239044 0.70114943 0.3        0.        ]\n","fscore   : [0.92073171 0.68539326 0.16216216 0.        ]\n","support  : [502  87  10   1]\n","\n","-----Report for task 2:-----\n","\n","precision: [0.89090909 0.58064516 0.2238806 ]\n","recall   : [0.88687783 0.39130435 0.75      ]\n","fscore   : [0.88888889 0.46753247 0.34482759]\n","support  : [442 138  20]\n"]}]},{"cell_type":"code","metadata":{"id":"HPpGBDuFRMcl"},"source":["RESULT_PATH = \"/content/drive/My Drive/Colab Notebooks/Bert Hate Speech/Results/\"\n","\n","specific_res = get_specific_results(results)\n","specific_res.to_pickle(RESULT_PATH + model_str + '_specific.pkl')\n","\n","macro_res    = get_macro_results(results)\n","macro_res.to_pickle(RESULT_PATH + model_str + '_macro.pkl')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3V4slhXyCB_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631126481182,"user_tz":420,"elapsed":120,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"3ada5b94-af76-4c56-fa2c-9fa51d859e08"},"source":["specific_res"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Normal</th>\n","      <th>Abusive</th>\n","      <th>Hate</th>\n","      <th>Neither/NA</th>\n","      <th>Anti-Asian</th>\n","      <th>Anti-Black</th>\n","      <th>Both</th>\n","      <th>No.Agg.</th>\n","      <th>Some.Agg.</th>\n","      <th>Very.Agg.</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.94 +- 0.01</td>\n","      <td>0.37 +- 0.04</td>\n","      <td>0.4 +- 0.1</td>\n","      <td>0.96 +- 0.01</td>\n","      <td>0.64 +- 0.04</td>\n","      <td>0.1 +- 0.02</td>\n","      <td>--</td>\n","      <td>0.89 +- 0.01</td>\n","      <td>0.58 +- 0.02</td>\n","      <td>0.38 +- 0.22</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.86 +- 0.03</td>\n","      <td>0.45 +- 0.2</td>\n","      <td>0.54 +- 0.23</td>\n","      <td>0.86 +- 0.03</td>\n","      <td>0.83 +- 0.07</td>\n","      <td>0.32 +- 0.07</td>\n","      <td>--</td>\n","      <td>0.89 +- 0.02</td>\n","      <td>0.56 +- 0.1</td>\n","      <td>0.41 +- 0.2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.9 +- 0.01</td>\n","      <td>0.38 +- 0.07</td>\n","      <td>0.41 +- 0.05</td>\n","      <td>0.91 +- 0.01</td>\n","      <td>0.72 +- 0.02</td>\n","      <td>0.14 +- 0.03</td>\n","      <td>--</td>\n","      <td>0.89 +- 0.01</td>\n","      <td>0.57 +- 0.06</td>\n","      <td>0.33 +- 0.06</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Normal       Abusive  ...     Some.Agg.     Very.Agg.\n","0  0.94 +- 0.01  0.37 +- 0.04  ...  0.58 +- 0.02  0.38 +- 0.22\n","1  0.86 +- 0.03   0.45 +- 0.2  ...   0.56 +- 0.1   0.41 +- 0.2\n","2   0.9 +- 0.01  0.38 +- 0.07  ...  0.57 +- 0.06  0.33 +- 0.06\n","\n","[3 rows x 10 columns]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"BbUQOvoq78Ru","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631126481337,"user_tz":420,"elapsed":159,"user":{"displayName":"Huy Nghiem","photoUrl":"","userId":"06903637911945605583"}},"outputId":"ad6f458e-75db-43f2-9abf-0ca05dcac3a1"},"source":["macro_res"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.57 +- 0.03</td>\n","      <td>0.62 +- 0.02</td>\n","      <td>0.59 +- 0.01</td>\n","      <td>0.8 +- 0.02</td>\n","      <td>0.85 +- 0.03</td>\n","      <td>0.82 +- 0.01</td>\n","      <td>0.62 +- 0.08</td>\n","      <td>0.62 +- 0.04</td>\n","      <td>0.62 +- 0.04</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              0             1  ...             7             8\n","0  0.57 +- 0.03  0.62 +- 0.02  ...  0.62 +- 0.04  0.62 +- 0.04\n","\n","[1 rows x 9 columns]"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"iVVd1MAOEMVg"},"source":[""],"execution_count":null,"outputs":[]}]}